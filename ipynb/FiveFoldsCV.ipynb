{"cells":[{"metadata":{"id":"6gdERXxK7TSe","colab_type":"code","colab":{},"trusted":true,"_uuid":"9eac36f616fa762c8a8b885198d567c2af4f5e1e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"mMnLbPmpsexF","colab_type":"text","_uuid":"3a338c7c7ee69d2caa9ceb61e428f44e5fa4c841"},"cell_type":"markdown","source":"# Overview\nInitial data-load code and submit code take from this awesome kernel https://www.kaggle.com/kmader/baseline-u-net-model-part-1 \n\n"},{"metadata":{"id":"nldNukoxsexH","colab_type":"code","colab":{},"trusted":true,"_uuid":"60ddb528be0c5856d1a709ece36515388d3a14ee"},"cell_type":"code","source":"BATCH_SIZE = 16#32#64#16#64#128#400#512#64#256#128#64#16\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\nfrom skimage.segmentation import mark_boundaries\n#from skimage.util.montage import montage2d as montage\nimport cv2\nimport random\nfrom datetime import datetime\nimport json\nimport gc\n\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.backends.cudnn as cudnn\nimport torch.backends.cudnn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom torchvision.transforms import ToTensor, Normalize, Compose\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nfrom pathlib import Path\n\nfrom skimage.morphology import label\nfrom skimage.transform import resize\n\nship_dir = '../input/tgs-salt-identification-challenge/'#'./drive/My Drive/salt/'#'../input/tgs-salt-identification-challenge/'#../input/airbus-ship-detection/'#../input/tgs-salt-identification-challenge/'#\ntrain_image_dir = os.path.join(ship_dir, 'train')\ntest_image_dir = os.path.join(ship_dir, 'test')\n# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef multi_rle_encode(img):\n    labels = label(img)\n    return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n\ndef rle_decode(mask_rle, shape=(101, 101)):#############################rle_decode(mask_rle, shape=(768, 768)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction\ndef upsample(img,img_size):\n    return cv2.resize(img.squeeze(), (img_size, img_size), interpolation=cv2.INTER_LINEAR)\n#cv2.resize(img, (img_size, img_size))#, mode='constant', preserve_range=True)\n\ndef upsamplearray(arr,img_size):\n    out=[upsample(x.squeeze(),img_size) for x in arr]\n    return np.array(out)    \n\ndef downsample(img,img_size):\n    #img_size=101\n    return cv2.resize(img.squeeze(), (img_size, img_size), interpolation=cv2.INTER_LINEAR)\n#resize(img, (img_size, img_size), mode='constant', preserve_range=True)\n\ndef downsamplearray(arr):\n    out=[downsample(x.squeeze(),101) for x in arr]\n    return np.array(out).reshape(-1,101,101,1)\n\ndef restore(img):\n    #return downsample(img.squeeze()[11:203,11:203])\n    img = downsample(img,128)\n    return img.squeeze()[13:114,13:114]#cv2.resize(img.squeeze(), (101, 101), interpolation=cv2.INTER_LINEAR)#img.squeeze()[13:114,13:114]#\n\ndef flpadding(img,a,b):#13,14\n    m,n=img.shape\n    output=np.zeros([m+a+b,n+a+b])#([127,127])\n    output[a:m+a,a:n+a]=img.squeeze()\n    imglr=np.fliplr(img.squeeze())\n    output[a:m+a,0:a]=imglr[:,-a:]\n    #output[13:114,-13:]=imglr[:,0:13]\n    output[a:m+a,-b:]=imglr[:,0:b]\n    imgud1=np.flipud(output)\n    output[0:a,:]=imgud1[-2*a:-a,:]\n    output[-b:,:]=imgud1[b:2*b,:]  \n    return output\n\ndef imgexpand0(img):\n    t_size=128#224\n    return upsample(flpadding(img.squeeze(),13,14),t_size).reshape(t_size,t_size,1)\n    #return flpadding(img.squeeze(),13,14).reshape(t_size,t_size,1)\n    \ndef imgexarray0(arr):\n    out=[imgexpand0(x) for x in arr]\n    return np.array(out)\n\ndef imgexpand03(img):\n    t_size=128#224\n    output0=np.zeros([t_size,t_size,3])#([192,192,3])#([127,127])\n    output=imgexpand0(img.squeeze())#np.zeros([128,128])#\n    for i in range(3):\n        output0[:,:,i]=output.squeeze()\n    return output0\n\ndef imgexarray03(arr):\n    out=[imgexpand03(x) for x in arr]\n    return np.array(out)\n\ndef imgexpand033(img):\n    t_size=128#224\n    output0=np.zeros([t_size,t_size,3])#([192,192,3])#([127,127])\n    output=imgexpand0(img[:,:,0].squeeze())#np.zeros([128,128])#\n    for i in range(3):\n        output0[:,:,i]=output.squeeze()\n    return output0\n\n\ndef masks_as_image(in_mask_list):\n    # Take the individual ship masks and create a single mask array for all ships\n    #all_masks = np.zeros((768, 768), dtype = np.int16)###################################################\n    all_masks = np.zeros((101, 101), dtype = np.int16)\n    #if isinstance(in_mask_list, list):\n    for mask in in_mask_list:\n        if isinstance(mask, str):\n            all_masks += rle_decode(mask)\n    return np.expand_dims(all_masks, -1)\n\ndef mask_overlay(image, mask, color=(0, 1, 0)):\n    \"\"\"\n    Helper function to visualize mask on the top of the image\n    \"\"\"\n    mask = np.dstack((mask, mask, mask)) * np.array(color)\n    weighted_sum = cv2.addWeighted(mask, 0.5, image, 0.5, 0.)\n    img = image.copy()\n    ind = mask[:, :, 1] > 0\n    img[ind] = weighted_sum[ind]    \n    return img\n\ndef imshow(img, mask, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    img = img.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    img = std * img + mean\n    img = np.clip(img, 0, 1)\n    mask = mask.numpy().transpose((1, 2, 0))\n    mask = np.clip(mask, 0, 1)\n    fig = plt.figure(figsize = (6,6))\n    plt.imshow(mask_overlay(img, mask))\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001) ","execution_count":null,"outputs":[]},{"metadata":{"id":"hO6XRiVEsexN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b9286ced-19e4-40b6-e051-fc08bcde06ba","executionInfo":{"status":"ok","timestamp":1537771425489,"user_tz":-480,"elapsed":559,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"0aab44556f04fc220a6df0139504a914cc5221ec"},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"id":"nJK8RFdtsexT","colab_type":"code","colab":{},"trusted":true,"_uuid":"5644173096a24206eef4f13d1d3f2df1abf8895f"},"cell_type":"code","source":"from tqdm import tqdm_notebook\nfrom keras.preprocessing.image import load_img\n\n\ntrain_df11 = pd.read_csv(\"../input/tgs-salt-identification-challenge/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df11 = pd.read_csv(\"../input/tgs-salt-identification-challenge/depths.csv\", index_col=\"id\")\n#train_df\ntrain_df11 = train_df11.join(depths_df11)\ntest_df11 = depths_df11[~depths_df11.index.isin(train_df11.index)]\n\n\n\n\n\ndf_pred = pd.read_csv('../input/tgs-salt-identification-challenge/train.csv', index_col=[0])\ndf_pred.fillna('', inplace=True)\ndf_pred['suspicious'] = False\ni=0\nfor index, row in df_pred.iterrows():\n    encoded_mask = row['rle_mask'].split(' ')\n    i=i+1\n    mask0 = rle_decode(row['rle_mask'])\n    #if i==4:\n     #   print(np.array(mask0).shape)\n     #   plt.imshow(mask0) #\n    ##remove small masks\n    #s=sum(sum(mask0))\n    #if s<30 and s>0:\n    #    df_pred.loc[index,'suspicious'] = True\n    \n    \n    #print(np.array(encoded_mask).shape)\n    #print(encoded_mask[1])df_pred.loc[index,'suspicious'] = True\n    if (len(encoded_mask) > 1 and len(encoded_mask) < 5 and int(encoded_mask[1]) % 101 == 0 and int(encoded_mask[1]) <= 100*101):\n        df_pred.loc[index,'suspicious'] = True\n        \nimg_size_ori = 101\nimg_size_target = 128#127\n\ntrain_df0 = pd.read_csv(\"../input/tgs-salt-identification-challenge/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df0 = pd.read_csv(\"../input/tgs-salt-identification-challenge/depths.csv\", index_col=\"id\")\n#train_df\ntrain_df0 = train_df0.join(depths_df0)\ntest_df0 = depths_df0[~depths_df0.index.isin(train_df0.index)]\ntrain_df0[\"images\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df0.index)]\n#print(train_df.shape) if ~df_pred.loc[idx,'suspicious']\ntrain_df0['suspicious'] = df_pred['suspicious']\ntrain_df0['rle_mask'] = df_pred['rle_mask']\ntrain_df0[\"masks\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df0.index)]\n\ntrain_df0[\"coverage\"] = train_df0.masks.map(np.sum) / pow(img_size_ori, 2)\n\ndef cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df0[\"coverage_class\"] = train_df0.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c63a1b3d9180590856165f3146d90ab42a8ab80"},"cell_type":"code","source":"train_df1=train_df0.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"TRylWW4Ssexf","colab_type":"code","colab":{},"trusted":true,"_uuid":"373a74a28c83424e6f9aab1297d76ea4369eb649"},"cell_type":"code","source":"train_df0.reset_index(inplace=True)\n\ntrain_df=train_df0.copy()\nvalid_df=train_df0.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9da00bd757df7f9509066e623293e57f4abcf36"},"cell_type":"code","source":"train_df.drop(['images','suspicious','coverage','masks'],axis=1,inplace=True)\nvalid_df.drop(['images','suspicious','coverage','masks'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"FefIJ6_5sexm","colab_type":"code","colab":{},"trusted":true,"_uuid":"591de9826f0aa76440fdc2384b99c5b919c11caf"},"cell_type":"code","source":"grp = list(train_df.groupby('id'))","execution_count":null,"outputs":[]},{"metadata":{"id":"eKHQ-6cqsexq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"da2b9741-cc66-4605-ac22-ef75835557ee","executionInfo":{"status":"ok","timestamp":1537773857983,"user_tz":-480,"elapsed":3306,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"0886851e1f7cc88cf3b3f75517083ea7c52df662"},"cell_type":"code","source":"zmin=min([list(m['z'].values) for _,m in grp])\nzmax=max([list(m['z'].values) for _,m in grp])\nzdif=(zmax[0]-zmin[0])\nprint(zdif)\nprint(zmin)","execution_count":null,"outputs":[]},{"metadata":{"id":"1zRAeYXRsexw","colab_type":"text","_uuid":"fd0297f5be89ee5f14305b1381e6724e2a208cb1"},"cell_type":"markdown","source":"## Dataset class for PyThorch dataloader"},{"metadata":{"id":"bcrgzdoZsexx","colab_type":"code","colab":{},"trusted":true,"_uuid":"92ee431236127bd4f2873cdd9311f17fb6071630"},"cell_type":"code","source":"def imgexpand03depth(img,depth):\n    t_size=128#224\n    output0=np.zeros([t_size,t_size,3])#([192,192,3])#([127,127])\n    output=imgexpand0(img.squeeze()).squeeze()#np.zeros([128,128])#\n    output0[:,:,0]=output\n    output0[:,:,1]=output*depth\n    output0[:,:,2]=output*depth\n    return output0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"291db2d8f61a918d0f138012eda3f96d7d77d171"},"cell_type":"code","source":"def imgexpand03d(img):\n    t_size=128#224\n    output0=np.zeros([t_size,t_size,3])#([192,192,3])#([127,127])\n    output=imgexpand0(img.squeeze()).squeeze()#np.zeros([128,128])#\n    output0[:,:,0]=output\n    output0[:,:,1]=output\n    output0[:,:,2]=output\n    return output0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7f73245687beae9d43637a58b2aeef0c39bd1d9"},"cell_type":"code","source":"def add_depth_channels(image_tensor):\n    h, w,_ = image_tensor.shape#size()\n    for row, const in enumerate(np.linspace(0, 1, h)):\n        image_tensor[row, :,1] = const\n    image_tensor[:,:,2] = image_tensor[:,:,0] * image_tensor[:,:,1]\n    return image_tensor\n\ndef C_add_depth_channels(image_tensor,depth):\n    h, w,_ = image_tensor.shape#size()\n    for row, const in enumerate(np.linspace(0, 1, h)):\n        image_tensor[row, :,1] = const\n    image_tensor[:,:,2] = image_tensor[:,:,0] * image_tensor[:,:,1]\n    image_tensor[:,:,1] = image_tensor[:,:,1] * depth\n    return image_tensor","execution_count":null,"outputs":[]},{"metadata":{"id":"mOocIpyzsex0","colab_type":"code","colab":{},"trusted":true,"_uuid":"7b0fd88373752f75d10a2963e625fe29257a2d38"},"cell_type":"code","source":"class SaltDataset(Dataset):\n    def __init__(self, in_df, transform=None, mode='train', zdif=908, zmin=51):\n        grp = list(in_df.groupby('id'))\n        \n        self.image_ids =  [_id for _id, _ in grp] \n        self.z = [m['z'].values for _,m in grp]\n        self.zmin = zmin\n        self.zdif = zdif\n        self.image_masks = [m['rle_mask'].values for _,m in grp]\n        self.transform = transform\n        self.mode = mode\n        self.img_transform = Compose([\n        ToTensor(),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    def __len__(self):\n        return len(self.image_ids)\n               \n    def __getitem__(self, idx):\n        img_file_name = self.image_ids[idx]\n        if self.mode == 'train':\n            rgb_path = os.path.join(train_image_dir, 'images',img_file_name+'.png')\n        elif self.mode == 'valid':\n            rgb_path = os.path.join(train_image_dir, 'images',img_file_name+'.png')\n        else:\n            if img_file_name[-4:]=='.png':\n                rgb_path = os.path.join(test_image_dir, 'images',img_file_name)\n            else:\n                rgb_path = os.path.join(test_image_dir, 'images',img_file_name+'.png')\n        img = imread(rgb_path)\n        mask = masks_as_image(self.image_masks[idx])  \n        depth = self.z[idx]\n        depth = math.ceil((depth-self.zmin)/self.zdif*101.)/101.\n        if depth==0:\n            depth=1./101.\n        #print(depth)\n        if self.transform is not None:\n            img, mask = self.transform(img, mask)\n            \n        img = imgexpand03d(img[:,:,0])\n        img = add_depth_channels(img)#\n        size0=192\n        img = cv2.resize(img[:, :, :], (size0, size0), interpolation=cv2.INTER_LINEAR)\n        \n        mask =imgexpand0(mask)#imgexpand256(mask)###################################################################\n        mask = cv2.resize(mask, (size0, size0), interpolation=cv2.INTER_LINEAR)    \n        \n        if mask is not None:           \n            if mask.ndim == 2:\n                mask = np.expand_dims(mask, axis=2)            \n        if img is not None:           \n            if img.ndim == 2:\n                img = np.expand_dims(img, axis=2)                   \n                \n        if self.mode == 'train':\n            #return self.to_float_tensor(img), self.to_float_tensor(mask)\n            #eturn img, mask\n            #print(mask.shape)\n            return self.img_transform(img), torch.from_numpy(np.moveaxis(mask, -1, 0)).float()\n        else:\n            return self.img_transform(img), str(img_file_name)","execution_count":null,"outputs":[]},{"metadata":{"id":"NCMaZYB6sex3","colab_type":"text","_uuid":"81848d0d9b7e80b7fb542f363840b271db9c7ca7"},"cell_type":"markdown","source":"## Check some image from validation dataset"},{"metadata":{"id":"cZrbqpRrsex5","colab_type":"code","colab":{},"trusted":true,"_uuid":"d1981f11b000e14a5b01883192c25ec76b9ab6b4"},"cell_type":"code","source":"#Create dataset\n#dataset_valid = ShipDataset(train_df)\ndataset_valid = SaltDataset(valid_df)\n\n#dataset_valid_sus = SaltDataset(valid_df_sus)\n#Create dataset\n#dataset_valid = ShipDataset(valid_df)","execution_count":null,"outputs":[]},{"metadata":{"id":"r28eO1iVsex_","colab_type":"text","_uuid":"362081a2133b76332d5989112a3b88627dd32886"},"cell_type":"markdown","source":"#To show image with its mask\nimshow(*dataset_valid[0])\nprint(*dataset_valid[0])"},{"metadata":{"id":"w2QvO8DgseyB","colab_type":"text","_uuid":"ea29caaa4f7ac0cb9a0c551e632b160f00fe0334"},"cell_type":"markdown","source":"# Augment Data\n\nWe should create custom classes for sumultaneous transformation images and  masks"},{"metadata":{"id":"Ls357AyiseyC","colab_type":"code","colab":{},"trusted":true,"_uuid":"9353bbdab6ed2e596d62df358143c57b1ec96525"},"cell_type":"code","source":"\"\"\"\n    Implementation from  https://github.com/ternaus/robot-surgery-segmentation\n\"\"\"\n\ndef clip(img, dtype, maxval):\n    return np.clip(img, 0, maxval).astype(dtype)\n\nclass DualCompose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, x, mask=None):\n        for t in self.transforms:\n            x, mask = t(x, mask)\n            if mask is not None:           \n                if mask.ndim == 2:\n                    mask = np.expand_dims(mask, axis=2)\n        return x, mask\n    \nclass OneOf:\n    def __init__(self, transforms, prob=0.5):\n        self.transforms = transforms\n        self.prob = prob\n\n    def __call__(self, x, mask=None):\n        if random.random() < self.prob:\n            t = random.choice(self.transforms)\n            t.prob = 1.\n            x, mask = t(x, mask)\n            #print(x.shape)\n            #print(mask.shape)\n            if mask is not None:           \n                if mask.ndim == 2:\n                    mask = np.expand_dims(mask, axis=2)\n        return x, mask\n\nclass OneOrOther:\n    def __init__(self, first, second, prob=0.5):\n        self.first = first\n        first.prob = 1.\n        self.second = second\n        second.prob = 1.\n        self.prob = prob\n\n    def __call__(self, x, mask=None):\n        if random.random() < self.prob:\n            x, mask = self.first(x, mask)\n        else:\n            x, mask = self.second(x, mask)\n        if mask is not None:           \n            if mask.ndim == 2:\n                mask = np.expand_dims(mask, axis=2)\n        return x, mask\n\n\nclass ImageOnly:\n    def __init__(self, trans):\n        self.trans = trans\n\n    def __call__(self, x, mask=None):\n        return self.trans(x), mask\n\n\nclass VerticalFlip:\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            img = cv2.flip(img, 0)\n            if mask is not None:\n                mask = cv2.flip(mask, 0)\n        return img, mask\n\n\nclass HorizontalFlip:\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            img = cv2.flip(img, 1)\n            if mask is not None:\n                mask = cv2.flip(mask, 1)\n                if mask.ndim == 2:\n                    mask = np.expand_dims(mask, axis=2)\n        return img, mask\n\n\nclass RandomFlip:\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            d = random.randint(-1, 1)\n            img = cv2.flip(img, d)\n            if mask is not None:\n                mask = cv2.flip(mask, d)\n                if mask.ndim == 2:\n                    mask = np.expand_dims(mask, axis=2)\n        return img, mask\n\n\nclass Transpose:\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            img = img.transpose(1, 0, 2)\n            if mask is not None:\n                mask = mask.transpose(1, 0, 2)\n        return img, mask\n\n\nclass RandomRotate90:\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            factor = random.randint(0, 4)\n            img = np.rot90(img, factor)\n            if mask is not None:\n                mask = np.rot90(mask, factor)\n        return img.copy(), mask.copy()\n\n\nclass Rotate:\n    def __init__(self, limit=90, prob=0.5):\n        self.prob = prob\n        self.limit = limit\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            angle = random.uniform(-self.limit, self.limit)\n\n            height, width = img.shape[0:2]\n            mat = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1.0)\n            img = cv2.warpAffine(img, mat, (height, width),\n                                 flags=cv2.INTER_LINEAR,\n                                 borderMode=cv2.BORDER_REFLECT_101)\n            if mask is not None:\n                if mask.ndim == 2:\n                    mask = np.expand_dims(mask, axis=2)\n                mask = cv2.warpAffine(mask, mat, (height, width),\n                                      flags=cv2.INTER_LINEAR,\n                                      borderMode=cv2.BORDER_REFLECT_101)\n\n        return img, mask\n\n\nclass RandomCrop:\n    def __init__(self, size):\n        self.h = size[0]\n        self.w = size[1]\n\n    def __call__(self, img, mask=None):\n        height, width, _ = img.shape\n\n        h_start = np.random.randint(0, height - self.h)\n        w_start = np.random.randint(0, width - self.w)\n\n        img = img[h_start: h_start + self.h, w_start: w_start + self.w,:]\n\n        assert img.shape[0] == self.h\n        assert img.shape[1] == self.w\n\n        if mask is not None:\n            if mask.ndim == 2:\n                mask = np.expand_dims(mask, axis=2)\n            mask = mask[h_start: h_start + self.h, w_start: w_start + self.w,:]\n\n        return img, mask\n\n\nclass Shift:\n    def __init__(self, limit=4, prob=.5):\n        self.limit = limit\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            limit = self.limit\n            dx = round(random.uniform(-limit, limit))\n            dy = round(random.uniform(-limit, limit))\n\n            height, width, channel = img.shape\n            y1 = limit + 1 + dy\n            y2 = y1 + height\n            x1 = limit + 1 + dx\n            x2 = x1 + width\n\n            img1 = cv2.copyMakeBorder(img, limit + 1, limit + 1, limit + 1, limit + 1,\n                                      borderType=cv2.BORDER_REFLECT_101)\n            img = img1[y1:y2, x1:x2, :]\n            if mask is not None:\n                msk1 = cv2.copyMakeBorder(mask, limit + 1, limit + 1, limit + 1, limit + 1,\n                                          borderType=cv2.BORDER_REFLECT_101)\n                mask = msk1[y1:y2, x1:x2, :]\n\n        return img, mask\n\n\nclass ShiftScale:\n    def __init__(self, limit=4, prob=.25):\n        self.limit = limit\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        limit = self.limit\n        if random.random() < self.prob:\n            height, width, channel = img.shape\n            assert (width == height)\n            size0 = width\n            size1 = width + 2 * limit\n            size = round(random.uniform(size0, size1))\n\n            dx = round(random.uniform(0, size1 - size))\n            dy = round(random.uniform(0, size1 - size))\n\n            y1 = dy\n            y2 = y1 + size\n            x1 = dx\n            x2 = x1 + size\n\n            img1 = cv2.copyMakeBorder(img, limit, limit, limit, limit, borderType=cv2.BORDER_REFLECT_101)\n            img = (img1[y1:y2, x1:x2, :] if size == size0\n            else cv2.resize(img1[y1:y2, x1:x2, :], (size0, size0), interpolation=cv2.INTER_LINEAR))\n\n            if mask is not None:\n                msk1 = cv2.copyMakeBorder(mask, limit, limit, limit, limit, borderType=cv2.BORDER_REFLECT_101)\n                mask = (msk1[y1:y2, x1:x2, :] if size == size0\n                else cv2.resize(msk1[y1:y2, x1:x2, :], (size0, size0), interpolation=cv2.INTER_LINEAR))\n\n        return img, mask\n\n\nclass ShiftScaleRotate:\n    def __init__(self, shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, prob=0.5):\n        self.shift_limit = shift_limit\n        self.scale_limit = scale_limit\n        self.rotate_limit = rotate_limit\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            height, width, channel = img.shape\n\n            angle = random.uniform(-self.rotate_limit, self.rotate_limit)\n            scale = random.uniform(1 - self.scale_limit, 1 + self.scale_limit)\n            dx = round(random.uniform(-self.shift_limit, self.shift_limit)) * width\n            dy = round(random.uniform(-self.shift_limit, self.shift_limit)) * height\n\n            cc = math.cos(angle / 180 * math.pi) * scale\n            ss = math.sin(angle / 180 * math.pi) * scale\n            rotate_matrix = np.array([[cc, -ss], [ss, cc]])\n\n            box0 = np.array([[0, 0], [width, 0], [width, height], [0, height], ])\n            box1 = box0 - np.array([width / 2, height / 2])\n            box1 = np.dot(box1, rotate_matrix.T) + np.array([width / 2 + dx, height / 2 + dy])\n\n            box0 = box0.astype(np.float32)\n            box1 = box1.astype(np.float32)\n            mat = cv2.getPerspectiveTransform(box0, box1)\n            img = cv2.warpPerspective(img, mat, (width, height),\n                                      flags=cv2.INTER_LINEAR,\n                                      borderMode=cv2.BORDER_REFLECT_101)\n            if mask is not None:\n                mask = cv2.warpPerspective(mask, mat, (width, height),\n                                           flags=cv2.INTER_NEAREST,\n                                           borderMode=cv2.BORDER_REFLECT_101)\n\n        return img, mask\n\n\nclass CenterCrop:\n    def __init__(self, size):\n        self.height = size[0]\n        self.width = size[1]\n\n    def __call__(self, img, mask=None):\n        h, w, c = img.shape\n        dy = (h - self.height) // 2\n        dx = (w - self.width) // 2\n        y1 = dy\n        y2 = y1 + self.height\n        x1 = dx\n        x2 = x1 + self.width\n        img = img[y1:y2, x1:x2,:]\n        if mask is not None:\n            if mask.ndim == 2:\n                mask = np.expand_dims(mask, axis=2)\n            mask = mask[y1:y2, x1:x2,:]\n\n        return img, mask\n    \nclass RandomBrightness:\n    def __init__(self, limit=0.1, prob=0.5):\n        self.limit = limit\n        self.prob = prob\n\n    def __call__(self, img, mask=None):\n        if random.random() < self.prob:\n            alpha = 1.0 + self.limit * random.uniform(-1, 1)\n\n            maxval = np.max(img[..., :3])\n            dtype = img.dtype\n            img[..., :3] = clip(alpha * img[..., :3], dtype, maxval)\n            if mask is not None:           \n                if mask.ndim == 2:\n                    mask = np.expand_dims(mask, axis=2)\n        return img,mask\n\n\nclass RandomContrast:\n    def __init__(self, limit=.1, prob=.5):\n        self.limit = limit\n        self.prob = prob\n\n    def __call__(self, img):\n        if random.random() < self.prob:\n            alpha = 1.0 + self.limit * random.uniform(-1, 1)\n\n            gray = cv2.cvtColor(img[:, :, :3], cv2.COLOR_BGR2GRAY)\n            gray = (3.0 * (1.0 - alpha) / gray.size) * np.sum(gray)\n            maxval = np.max(img[..., :3])\n            dtype = img.dtype\n            img[:, :, :3] = clip(alpha * img[:, :, :3] + gray, dtype, maxval)\n        return img\n\nclass Imgexpand:\n    def __init__(self, s=101,e=128):\n        self.s = s\n        self.e = e\n\n    def __call__(self, img, mask):\n        output0=np.zeros([128,128,3])#([127,127])\n        mask0=np.zeros([128,128,1])#([127,127])\n        output=np.zeros([128,128])#\n        #for i in range(3)\n        output[13:114,13:114]=img[:,:,0]\n        imglr=np.fliplr(img[:,:,0])\n        output[13:114,0:13]=imglr[:,-13:]\n        #output[13:114,-13:]=imglr[:,0:13]\n        output[13:114,-14:]=imglr[:,0:14]\n        imgud1=np.flipud(output)\n        output[0:13,:]=imgud1[-26:-13,:]\n        output[-14:,:]=imgud1[14:28,:] \n        #output[-13:,:]=imgud1[13:26,:]  \n        #img=output\n        for i in range(3):\n            output0[:,:,i]=output\n            \n        output[13:114,13:114]=mask.squeeze()\n        imglr=np.fliplr(img[:,:,0])\n        output[13:114,0:13]=imglr[:,-13:]\n        #output[13:114,-13:]=imglr[:,0:13]\n        output[13:114,-14:]=imglr[:,0:14]\n        imgud1=np.flipud(output)\n        output[0:13,:]=imgud1[-26:-13,:]\n        output[-14:,:]=imgud1[14:28,:]\n        mask0[:,:,-1]=output\n        return output0,mask0\n    \nclass ZoominRandomCrop:\n    def __init__(self, size, scale, prob=0.5):\n        self.h = size[0]#101\n        self.w = size[1]#101\n        self.s = scale\n        self.prob = prob\n        #self.ori=ori\n    def __call__(self, img, mask=None):\n        #img = img[13: 114, 13: 114,:]\n        if random.random() < self.prob: \n            #y1=13\n            #y2=114\n            #x1=13\n            #x2=114\n            hw=int(self.h*self.s)\n            img1=cv2.resize(img, (hw, hw), interpolation=cv2.INTER_LINEAR)\n            #img1=cv2.resize(img[y1:y2, x1:x2, :], (hw, hw), interpolation=cv2.INTER_LINEAR)\n            #height, width, _ = 120#img.shape\n            height =hw\n            width  =hw\n\n            h_start = np.random.randint(0, height - self.h)\n            w_start = np.random.randint(0, width - self.w)\n\n            img = img1[h_start: h_start + self.h, w_start: w_start + self.w,:]\n            #img = imgexpand033(img1)\n\n            #assert img.shape[0] == self.h\n            #assert img.shape[1] == self.w\n\n            if mask is not None:\n                if mask.ndim == 2:\n                    mask = np.expand_dims(mask, axis=2)\n                #print(mask.shape)\n                mask1=cv2.resize(mask, (hw, hw), interpolation=cv2.INTER_LINEAR)\n                if mask1.ndim == 2:\n                    mask1 = np.expand_dims(mask1, axis=2)                \n                #print(mask1.shape)\n                mask = mask1[h_start: h_start + self.h, w_start: w_start + self.w,:]\n\n        return img, mask\n    \nclass ZoominRandomCropNew:\n    def __init__(self, size, scale, prob=0.5):\n        self.h = size[0]#101\n        self.w = size[1]#101\n        self.s = scale\n        self.prob = prob\n        #self.ori=ori\n    def __call__(self, img, mask=None):\n        #img = img[13: 114, 13: 114,:]\n        if random.random() < self.prob: \n            #y1=13\n            #y2=114\n            #x1=13\n            #x2=114\n            #hw=int(self.h*self.s)\n            sc=random.random()*(self.s-1)+1\n            hw=int(self.h*sc)+1\n            img1=cv2.resize(img, (hw, hw), interpolation=cv2.INTER_LINEAR)\n            #img1=cv2.resize(img[y1:y2, x1:x2, :], (hw, hw), interpolation=cv2.INTER_LINEAR)\n            #height, width, _ = 120#img.shape\n            height =hw\n            width  =hw\n            #print(hw)\n            h_start = np.random.randint(0, height - self.h)\n            w_start = np.random.randint(0, width - self.w)\n\n            img = img1[h_start: h_start + self.h, w_start: w_start + self.w,:]\n            #img = imgexpand033(img1)\n\n            #assert img.shape[0] == self.h\n            #assert img.shape[1] == self.w\n\n            if mask is not None:\n                if mask.ndim == 2:\n                    mask = np.expand_dims(mask, axis=2)\n                #print(mask.shape)\n                mask1=cv2.resize(mask, (hw, hw), interpolation=cv2.INTER_LINEAR)\n                if mask1.ndim == 2:\n                    mask1 = np.expand_dims(mask1, axis=2)                \n                #print(mask1.shape)\n                mask = mask1[h_start: h_start + self.h, w_start: w_start + self.w,:]\n\n        return img, mask","execution_count":null,"outputs":[]},{"metadata":{"id":"dQE-fgXnseyF","colab_type":"code","colab":{},"trusted":true,"_uuid":"3a3563eb7e960d1858c650f7865909bf970e8c65"},"cell_type":"code","source":"train_transform = DualCompose([\n        HorizontalFlip(),\n        ZoominRandomCropNew([101,101],1.10),\n        Rotate(limit=5),\n        RandomBrightness(limit=0.08),\n])\n\nval_transform = DualCompose([\n        #Imgexpand(),##############################################CenterCrop((512,512,3)),\n      ])","execution_count":null,"outputs":[]},{"metadata":{"id":"DIWt9fkQseyJ","colab_type":"text","_uuid":"335b7f350347270ebb84a8d6d4c4910e47222114"},"cell_type":"markdown","source":"# UNET model"},{"metadata":{"id":"pJXpUNlsseyK","colab_type":"code","colab":{},"trusted":true,"_uuid":"f3534b1afb0dd8729bc071f19fcbd57243b03106"},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchvision import models\nimport torch.nn.functional as F\nimport math\nimport torch.utils.model_zoo as model_zoo\n\nnonlinearity = nn.ReLU\nclass EncoderBlock(nn.Module):\n    def __init__(self, inchannel, outchannel, stride):\n        super().__init__()\n        \n        self.c1=nn.Conv2d(inchannel, outchannel, 3, stride, 1, bias=False)\n        self.bn1=nn.BatchNorm2d(outchannel)\n        self.re1=nn.ReLU(inplace=True)\n        self.c2=nn.Conv2d(outchannel, outchannel, 3, 1, 1, bias=False)\n        self.bn2=nn.BatchNorm2d(outchannel)\n        self.re2=nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.c1(x)\n        x = self.bn1(x)\n        x = self.re1(x)\n        x = self.c2(x)\n        x = self.bn2(x)\n        x = self.re2(x)\n        return x\nclass EncoderBlock0(nn.Module):\n    def __init__(self, in_channels, n_filters):\n        super().__init__()\n\n        # B, C, H, W -> B, C/4, H, W\n        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n        self.relu1 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C/4, H, W\n        self.pool = nn.MaxPool2d(2, 2)#\n        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n        self.relu2 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C, H, W\n        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n        self.norm3 = nn.BatchNorm2d(n_filters)\n        self.relu3 = nonlinearity(inplace=True)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu1(x)\n        x = self.pool(x)\n        x = self.norm2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        x = self.norm3(x)\n        x = self.relu3(x)\n        return x\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, n_filters):\n        super().__init__()\n\n        # B, C, H, W -> B, C/4, H, W\n        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n        self.relu1 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C/4, H, W\n        self.deconv2 = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, 3,\n                                          stride=2, padding=1, output_padding=1)\n        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n        self.relu2 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C, H, W\n        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n        self.norm3 = nn.BatchNorm2d(n_filters)\n        self.relu3 = nonlinearity(inplace=True)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu1(x)\n        x = self.deconv2(x)\n        x = self.norm2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        x = self.norm3(x)\n        x = self.relu3(x)\n        return x\n\nclass ChannelSE(nn.Module):\n    def __init__(self,inchannel):\n        super().__init__()\n        self.lin1=torch.nn.Linear(inchannel, inchannel//2)\n        self.lin2=torch.nn.Linear(inchannel//2, inchannel)\n        self.c=inchannel\n    def forward(self,x):\n        #_,c,h,w=x.size\n        #print(c)\n        #print(h)\n        #print(w)\n        m=torch.mean(torch.mean(x,dim=2,keepdim=True),dim=3,keepdim=True)\n        m = m.view(m.size(0), -1)\n        m=self.lin1(m)\n        m=nn.ReLU()(m)\n        m=self.lin2(m)\n\n        m=nn.Sigmoid()(m)\n        m = m.view(m.size(0), self.c,1,1)\n        x=m*x#torch.matmul(m,x)\n        return x\n\nclass SpatialSE(nn.Module):\n    def __init__(self,inchannel):\n        super().__init__()\n        self.conv=torch.nn.Conv2d(inchannel,1,kernel_size=1,stride=1)\n    def forward(self,x):\n        #_,c,h,w=x.size\n        #print(c)\n        #print(h)\n        #print(w)\n        m = self.conv(x)\n        m=nn.Sigmoid()(m)\n        x=m*x#torch.matmul(m,x)\n        return x\n\nclass DecoderBlockv(nn.Module):\n    def __init__(self, in_channels, n_filters):\n        super().__init__()\n\n        # B, C, H, W -> B, C/4, H, W\n        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n        self.relu1 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C/4, H, W\n        self.deconv2 = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, 3,\n                                          stride=2, padding=1, output_padding=1)\n        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n        self.relu2 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C, H, W\n        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n        self.norm3 = nn.BatchNorm2d(n_filters)\n        self.relu3 = nonlinearity(inplace=True)\n        \n        self.cSE = ChannelSE(n_filters)\n        self.sSE = SpatialSE(n_filters)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu1(x)\n        x = self.deconv2(x)\n        x = self.norm2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        x = self.norm3(x)\n        x = self.relu3(x)\n        x = self.cSE(x) + self.sSE(x)\n        return x\nclass ConvUp(nn.Module):\n    def __init__(self, in_channels, n_filters):\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2,mode='bilinear')\n        self.conv1 = nn.Conv2d(in_channels, n_filters, 3, padding = 1)\n        self.norm1 = nn.BatchNorm2d(n_filters)\n        self.relu1 = nonlinearity(inplace=True)\n\n    def forward(self, x):\n        x = self.upsample(x)\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu1(x)\n        return x   \nclass ConscSE(nn.Module):\n    def __init__(self, n_filters):\n        super().__init__()\n\n        \n        self.cSE = ChannelSE(n_filters)\n        self.sSE = SpatialSE(n_filters)\n        \n    def forward(self, x):\n        x = self.cSE(x) + self.sSE(x)\n        return x\n    \nclass DecoderBlockup(nn.Module):\n    def __init__(self, in_channels, n_filters):\n        super().__init__()\n\n        # B, C, H, W -> B, C/4, H, W\n        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n        self.relu1 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C/4, H, W\n        self.deconv2 = ConvUp(in_channels // 4, in_channels // 4)\n        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n        self.relu2 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C, H, W\n        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n        self.norm3 = nn.BatchNorm2d(n_filters)\n        self.relu3 = nonlinearity(inplace=True)\n        \n        self.cSE = ChannelSE(n_filters)\n        self.sSE = SpatialSE(n_filters)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu1(x)\n        x = self.deconv2(x)\n        x = self.norm2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        x = self.norm3(x)\n        x = self.relu3(x)\n        x = self.cSE(x) + self.sSE(x)\n        return x\n    \nclass DecoderBlock23(nn.Module):\n    def __init__(self, in_channels, n_filters, scal=4):\n        super().__init__()\n        self.up = nn.Upsample(scale_factor=2,mode='bilinear')\n        # B, C, H, W -> B, C/4, H, W\n        self.conv1 = nn.Conv2d(in_channels, in_channels // scal, 1)\n        self.norm1 = nn.BatchNorm2d(in_channels // scal)\n        self.relu1 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C, H, W\n        self.conv2 = nn.Conv2d(in_channels // scal, n_filters, 1)\n        self.norm2 = nn.BatchNorm2d(n_filters)\n        self.relu2 = nonlinearity(inplace=True)\n        \n        self.cSE = ChannelSE(n_filters)\n        self.sSE = SpatialSE(n_filters)\n        \n    def forward(self, x):\n        x = self.up(x)\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.norm2(x)\n        x = self.relu2(x)\n        #x = self.cSE(x) + self.sSE(x)\n        return x\n    \nclass Upscale:\n    transposed_conv = 0\n    upsample_bilinear = 1\n    pixel_shuffle = 2  \nclass BasicDecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels, conv_size=3, upscale=Upscale.transposed_conv):\n        super().__init__()\n        padding = 0\n        if conv_size == 3:\n            padding = 1\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels, middle_channels, conv_size, padding=padding),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(inplace=True)\n        )\n\n        last_conv_channels = middle_channels\n        if upscale == Upscale.transposed_conv:\n            self.layer2 = nn.Sequential(\n                nn.ConvTranspose2d(middle_channels, middle_channels, 3, stride=2, padding=1, output_padding=1),\n                nn.BatchNorm2d(middle_channels),\n                nn.ReLU(inplace=True)\n            )\n        elif upscale == Upscale.upsample_bilinear:\n            self.layer2 = nn.Upsample(scale_factor=2)\n        else:\n            self.layer2 = nn.PixelShuffle(upscale_factor=2)\n            last_conv_channels = middle_channels // 4\n\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(last_conv_channels, out_channels, conv_size, padding=padding),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return x\n\nclass UnetBNDecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels, upscale=Upscale.upsample_bilinear):\n        super().__init__()\n        self.layer = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.layer(x)\n    \nclass LinkNet34a(nn.Module):\n    def __init__(self, num_classes, num_channels=3):\n        super().__init__()\n        assert num_channels == 3, \"num channels not used now. to use changle first conv layer to support num channels other then 3\"\n        filters = [64, 128, 256, 512]\n        resnet = models.resnet34(pretrained=True)\n\n        self.firstconv = resnet.conv1\n        self.firstbn = resnet.bn1\n        self.firstrelu = resnet.relu\n        self.firstmaxpool = resnet.maxpool\n        self.encoder1 = resnet.layer1\n        self.encoder2 = resnet.layer2\n        self.encoder3 = resnet.layer3\n        self.encoder4 = resnet.layer4\n        # Center\n        self.center = nn.Sequential(\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(filters[3], filters[1], 3, padding=1),\n            nn.BatchNorm2d(filters[1]),\n            nn.ReLU(inplace=True)            \n        \n        )\n        # Decoder\n        self.decoder5 = UnetBNDecoderBlock(filters[1],filters[2]//4, filters[2])#\n        self.conv5=nn.Conv2d(256+512,256,1) \n        self.decoder4 = UnetBNDecoderBlock(filters[2],filters[2]//4, filters[1])#DecoderBlock(filters[3], filters[2])\n        self.conv4=nn.Conv2d(128+256,256,1)        \n        self.decoder3 = UnetBNDecoderBlock(filters[2],filters[2]//4, filters[0])#DecoderBlock(filters[2], filters[1])\n        self.conv3=nn.Conv2d(64+128,128,1)        \n        self.decoder2 = UnetBNDecoderBlock(filters[1],filters[1]//4, filters[0])#DecoderBlock(filters[1], filters[0])\n        self.conv2=nn.Conv2d(128,64,1)        \n        #self.decoder1 = UnetBNDecoderBlock(filters[0],filters[0]//4, filters[0])#DecoderBlock(filters[0], filters[0])\n\n        # Final Classifier\n        self.finaldeconv1 = UnetBNDecoderBlock(filters[0],filters[0]//4, filters[0])#ConvUp(filters[0], filters[0])\n        \n        # Final Classifier\n        self.logit = nn.Sequential(\n            nn.Conv2d(64, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nonlinearity(inplace=True),\n            nn.Conv2d(64, 1, 1),\n        )        \n\n\n    # noinspection PyCallingNonCallable\n    def forward(self, x):\n        # Encoder\n        x = x.float()\n        x = self.firstconv(x)\n        x = self.firstbn(x)\n        x = self.firstrelu(x)\n        #x = self.firstmaxpool(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n        ############################\n        e5 = self.center(e4)\n        d5 = torch.cat([self.decoder5(e5) , e4], 1)#concat([self.decoder5(e5) , e4])\n        d5 = self.conv5(d5)     \n        #########################\n        d4 = torch.cat([self.decoder4(d5) , e3], 1)#concat([self.decoder5(e5) , e4])\n        d4 = self.conv4(d4)\n        # d4 = e3\n        #d3 = self.decoder3(d4) + e2\n        #print(e2.shape)\n        d3 = torch.cat([self.decoder3(d4) , e2], 1)#concat([self.decoder5(e5) , e4])\n        #print(d3.shape)\n        d3 = self.conv3(d3)\n        \n        #d2 = self.decoder2(d3) + e1\n        d2 = torch.cat([self.decoder2(d3) , e1], 1)#concat([self.decoder5(e5) , e4])\n        d2 = self.conv2(d2)\n        \n        #d1 = self.decoder1(d2)\n\n        # Final Classification\n        f = self.finaldeconv1(d2)\n        #f = self.finalrelu1(f)\n        f = self.logit(f)\n        return f","execution_count":null,"outputs":[]},{"metadata":{"id":"rr5-wcooseyN","colab_type":"code","colab":{},"trusted":true,"_uuid":"28bce832747432128400b6603657a9d824654d3d"},"cell_type":"code","source":"class DecoderBlockH(nn.Module):\n    def __init__(self, in_channels,channels, n_filters):\n        super().__init__()\n        self.up = nn.Upsample(scale_factor=2,mode='bilinear')\n        \n        # B, C, H, W -> B, C/4, H, W\n        self.conv1 = nn.Conv2d(in_channels, channels, 3, padding=1)\n        self.norm1 = nn.BatchNorm2d(channels)\n        self.relu1 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C, H, W\n        self.conv2 = nn.Conv2d(channels, n_filters, 3, padding=1)\n        self.norm2 = nn.BatchNorm2d(n_filters)\n        self.relu2 = nonlinearity(inplace=True)\n        \n        #self.cSE = ChannelSE(n_filters)\n        #self.sSE = SpatialSE(n_filters)\n        \n    def forward(self, x, e=None):\n        x = self.up(x)\n        if e is not None:\n            x = torch.cat([x, e], 1)\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.norm2(x)\n        x = self.relu2(x)\n        #x = self.cSE(x) + self.sSE(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2fd9067d50bfe66ff03af2cf7c7916117d6b61f"},"cell_type":"code","source":"class GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        in_size = inputs.size()\n        inputs = inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n        return inputs.view((in_size[0], in_size[1], 1, 1))\n\n\n\n\n\n\n\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.fcs = nn.Sequential(nn.Linear(channel, int(channel/reduction)),\n                                 nn.LeakyReLU(negative_slope=0.1, inplace=True),\n                                 nn.Linear(int(channel/reduction), channel),\n                                 nn.Sigmoid())\n\n    def forward(self, x):\n        bahs, chs, _, _ = x.size()\n\n        # Returns a new tensor with the same data as the self tensor but of a different size.\n        y = self.avg_pool(x).view(bahs, chs)\n        y = self.fcs(y).view(bahs, chs, 1, 1)\n        return torch.mul(x, y)\n\n\nclass SCSEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SCSEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel//reduction)),\n                                                nn.ReLU(inplace=True),\n                                                nn.Linear(int(channel//reduction), channel),\n                                                nn.Sigmoid())\n\n        self.spatial_se = nn.Sequential(nn.Conv2d(channel, 1, kernel_size=1,\n                                                  stride=1, padding=0, bias=False),\n                                        nn.Sigmoid())\n\n    def forward(self, x):\n        bahs, chs, _, _ = x.size()\n\n        # Returns a new tensor with the same data as the self tensor but of a different size.\n        chn_se = self.avg_pool(x).view(bahs, chs)\n        chn_se = self.channel_excitation(chn_se).view(bahs, chs, 1, 1)\n        chn_se = torch.mul(x, chn_se)\n\n        spa_se = self.spatial_se(x)\n        spa_se = torch.mul(x, spa_se)\n        return torch.add(chn_se, 1, spa_se)\n\n\nclass ModifiedSCSEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(ModifiedSCSEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel//reduction)),\n                                                nn.ReLU(inplace=True),\n                                                nn.Linear(int(channel//reduction), channel),\n                                                nn.Sigmoid())\n\n        self.spatial_se = nn.Sequential(nn.Conv2d(channel, 1, kernel_size=1,\n                                                  stride=1, padding=0, bias=False),\n                                        nn.Sigmoid())\n\n    def forward(self, x):\n        bahs, chs, _, _ = x.size()\n\n        # Returns a new tensor with the same data as the self tensor but of a different size.\n        chn_se = self.avg_pool(x).view(bahs, chs)\n        chn_se = self.channel_excitation(chn_se).view(bahs, chs, 1, 1)\n\n        spa_se = self.spatial_se(x)\n        return torch.mul(torch.mul(x, chn_se), spa_se) \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9ee64cafbfd5a371dfcd13e7367694884a14f66"},"cell_type":"code","source":"class ConvBn2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n        super().__init__()\n        self.layer = nn.Sequential(\n            #nn.Upsample(scale_factor=2, mode='bilinear'),\n            nn.Conv2d(in_channels, out_channels, kernel_size, padding),\n            nn.BatchNorm2d(out_channels),\n            #nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.layer(x)\n    \nclass Decoder3(nn.Module):\n    def __init__(self, in_channels,res_channels, channels, n_filters):\n        super().__init__()\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear')\n        \n        # B, C, H, W -> B, C/4, H, W\n        self.conv1 = nn.Conv2d(in_channels+res_channels, channels, 3, padding=1)\n        self.norm1 = nn.BatchNorm2d(channels)\n        self.relu1 = nonlinearity(inplace=True)\n\n        # B, C/4, H, W -> B, C, H, W\n        self.conv2 = nn.Conv2d(channels, n_filters, 3, padding=1)\n        self.norm2 = nn.BatchNorm2d(n_filters)\n        self.relu2 = nonlinearity(inplace=True)\n        \n        self.SCSE = SCSEBlock(n_filters)#ChannelSE(n_filters)\n        #self.sSE = SpatialSE(n_filters)\n        \n    def forward(self, x, e=None):\n        x = self.up(x)\n        if e is not None:\n            x = torch.cat([x, e], 1)\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.norm2(x)\n        x = self.relu2(x)\n        x = self.SCSE(x)# + self.sSE(x)\n        return x\n    \nclass DenseNet34(nn.Module):\n    def __init__(self ):\n        super().__init__()\n        #super(Net,self).__init__()\n        filters = [64, 128, 256, 512]\n        self.resnet =  models.resnet34(pretrained=True)#ResNet(BasicBlock, [3, 4, 6, 3], num_classes=1 )\n\n        self.encoder1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n        )\n        self.encoder2 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            self.resnet.layer1,\n        )\n        self.encoder3 = self.resnet.layer2\n        self.encoder4 = self.resnet.layer3\n        self.encoder5 = self.resnet.layer4\n\n        self.center = nn.Sequential(\n            ConvBn2d( 512, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            ConvBn2d( 256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n######################################################################\n        #self.decoder5 = Decoder3(256, 512, 512, 64)\n        #self.decoder4 = Decoder3( 64, 256, 256, 64)\n        #self.decoder3 = Decoder3( 64, 128, 128, 64)\n        #self.decoder2 = Decoder3( 64,  64,  64, 64)\n        #self.decoder1 = Decoder3( 64,  64,  32, 64)\n        \n        self.decoder5 = DecoderBlockH(filters[3]+filters[2],filters[2], 64)\n        #self.conv5=nn.Conv2d(64+512,64,1)#before or after SE?\n        self.se5=SCSEBlock(64)\n        self.decoder4 = DecoderBlockH(filters[2]+64, filters[1], 64)\n        #self.conv4=nn.Conv2d(64+256,64,1)        \n        self.se4=SCSEBlock(64)\n        self.decoder3 = DecoderBlockH(filters[1]+64, filters[1], 64)\n        #self.conv3=nn.Conv2d(64+128,64,1)    \n        self.se3=SCSEBlock(64)\n        self.decoder2 = DecoderBlockH(filters[0]+64, filters[0], 64)\n        #self.conv2=nn.Conv2d(64+64,64,1)   \n        self.se2=SCSEBlock(64)\n        self.decoder1 = DecoderBlockH(filters[0], filters[0]//2, 64)\n        self.se1=SCSEBlock(64)        \n        \n##############################################################################        \n        self.fuse_pixel  = nn.Sequential(        \n            nn.Conv2d(320, 64, kernel_size=3, padding=1),\n        )\n        self.logit_pixel  = nn.Sequential(\n            #nn.Conv2d(320, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d( 64,  1, kernel_size=1, padding=0),\n        )\n\n        self.logit_image = nn.Sequential(\n            #nn.Linear(512, 128),\n            nn.ReLU(inplace=True),\n            #nn.Linear(128, 1),\n            nn.Linear(64, 1),\n        )\n        self.fuse_image = nn.Sequential(\n            nn.Linear(512, 64),\n            #nn.ReLU(inplace=True),\n            #nn.Linear(128, 1),\n        )\n        self.fuse = nn.Sequential(\n            #nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            #nn.Conv2d(128, 64, kernel_size=1, padding=0),\n            #nn.BatchNorm2d(64),\n            #nn.ReLU(inplace=True),\n        )\n        self.logit = nn.Sequential(\n            nn.Conv2d(128, 1, kernel_size=1, padding=0),\n            #nn.ReLU(inplace=True),\n            #nn.Conv2d( 64,  1, kernel_size=1, padding=0),\n        )\n\n    def forward(self, x):\n        batch_size,C,H,W = x.shape\n        \"\"\"\n        mean=[0.485, 0.456, 0.406]\n        std =[0.229, 0.224, 0.225]\n        x = torch.cat([\n            (x-mean[2])/std[2],\n            (x-mean[1])/std[1],\n            (x-mean[0])/std[0],\n        ],1)\n        \"\"\"\n        x = x.float()\n\n        e1 = self.encoder1(x )  #; print('e1',e1.size())\n        e2 = self.encoder2(e1)  #; print('e2',e2.size())\n        e3 = self.encoder3(e2)  #; print('e3',e3.size())\n        e4 = self.encoder4(e3)  #; print('e4',e4.size())\n        e5 = self.encoder5(e4)  #; print('e5',e5.size())\n\n        f = self.center(e5)                #; print('f',f.size())\n        #print(f.shape)\n        #print(e5.shape)\n        #e1 = self.encoder1(x)#\n        #e2 = self.encoder2(e1)#\n        #e3 = self.encoder3(e2)#\n        #e4 = self.encoder4(e3)#\n\n        #e5 = self.center(e4)#512\n        ####################################################################################\n        #d5 = self.decoder5( f,e5)          #; print('d5',f.size())\n        #d4 = self.decoder4(d5,e4)          #; print('d4',f.size())\n        #d3 = self.decoder3(d4,e3)          #; print('d3',f.size())\n        #d2 = self.decoder2(d3,e2)          #; print('d2',f.size())\n        #d1 = self.decoder1(d2,e1)          #; print('d1',f.size())\n        \n        d5 = self.decoder5(f,e5)\n        d5 = self.se5(d5)\n        # Decoder with Skip Connections\n        #d4 = self.decoder4(d5) + e3\n        #d4 = torch.cat([self.decoder4(d5) , e3], 1)#concat([self.decoder5(e5) , e4])\n        #print(d5.shape)\n        #print(e3.shape)\n        d4 = self.decoder4(d5,e4)\n        d4 = self.se4(d4)\n        # d4 = e3\n        #d3 = self.decoder3(d4) + e2\n        #print(e2.shape)\n        #d3 = torch.cat([self.decoder3(d4) , e2], 1)#concat([self.decoder5(e5) , e4])\n        #print(d3.shape)\n        d3 = self.decoder3(d4,e3)\n        d3 = self.se3(d3)\n        \n        #d2 = self.decoder2(d3) + e1\n        #d2 = torch.cat([self.decoder2(d3) , e1], 1)#concat([self.decoder5(e5) , e4])\n        d2 = self.decoder2(d3,e2)\n        d2 = self.se2(d2)\n        d1 = self.decoder1(d2)\n        d1 = self.se1(d1)\n        ########################################################################################\n        d = torch.cat((\n            d1,\n            F.upsample(d2,scale_factor= 2, mode='bilinear',align_corners=False),\n            F.upsample(d3,scale_factor= 4, mode='bilinear',align_corners=False),\n            F.upsample(d4,scale_factor= 8, mode='bilinear',align_corners=False),\n            F.upsample(d5,scale_factor=16, mode='bilinear',align_corners=False),\n        ),1)\n        \n        #######################################################################\n        \"\"\"\n        d = F.dropout(d, p=0.50, training=self.training)\n        logit_pixel = self.logit_pixel(d)\n\n        f = F.adaptive_avg_pool2d(e5, output_size=1).view(batch_size,-1)\n        f = F.dropout(f, p=0.50, training=self.training)\n        logit_image = self.logit_image(f).view(-1)\n        \"\"\"\n        ###########################################################################\n        #d = torch.cat([d1,d2,d3,d4,d5],1) #hyper-columns\n        d = F.dropout(d, p=0.50, training=self.training)\n        fuse_pixel  = self.fuse_pixel(d)#64-128-128\n        logit_pixel = self.logit_pixel(fuse_pixel)#1-128-128\n\n        e = F.adaptive_avg_pool2d(e5, output_size=1).view(batch_size,-1) #image pool#-512-1-1\n        e = F.dropout(e, p=0.50, training=self.training)#\n        fuse_image  = self.fuse_image(e)#-64-1-1\n        logit_image = self.logit_image(fuse_image).view(-1)#-1-1-1\n\n        #fuse = self.fuse(torch.mul(fuse_pixel, F.upsample(fuse_image.view(batch_size,-1,1,1,),scale_factor=128, mode='nearest')))\n        #fuse = self.fuse(fuse_pixel+ F.upsample(fuse_image.view(batch_size,-1,1,1,),scale_factor=128, mode='nearest'))\n        fuse = self.fuse(torch.cat([ #fuse\n            fuse_pixel,\n            F.upsample(fuse_image.view(batch_size,-1,1,1,),scale_factor=128, mode='nearest')\n        ],1))\n        logit = self.logit(fuse)#1-128-128\n\n        return logit, logit_pixel, logit_image        \n        \n        \n        #return logit_pixel, logit_image\n\n\n    ##-----------------------------------------------------------------\n\n\n    #def criterion(self, logit_pixel, logit_image, truth_pixel, truth_image, is_average=True):\n\n    \n    \n    \n\"\"\"\nd3 = F.upsample(d3,scale_factor= 4, mode='bilinear',align_corners=False)\n    d4 = F.upsample(d4,scale_factor= 8, mode='bilinear',align_corners=False)\n    d5 = F.upsample(d5,scale_factor=16, mode='bilinear',align_corners=False)\n\n    d = torch.cat([d1,d2,d3,d4,d5],1) #hyper-columns\n    d = F.dropout(d, p=0.50, training=self.training)\n    fuse_pixel  = self.fuse_pixel(d)\n    logit_pixel = self.logit_pixel(fuse_pixel)\n\n    e = F.adaptive_avg_pool2d(e5, output_size=1).view(batch_size,-1) #image pool\n    e = F.dropout(e, p=0.50, training=self.training)\n    fuse_image  = self.fuse_image(e)\n    logit_image = self.logit_image(fuse_image).view(-1)\n\n    fuse = self.fuse(torch.cat([ #fuse\n        fuse_pixel,\n        F.upsample(fuse_image.view(batch_size,-1,1,1,),scale_factor=128, mode='nearest')\n    ],1))\n    logit = self.logit(fuse)\n\n    return logit, logit_pixel, logit_image\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47abb6ecddf528affcfb6b942e7c8a39923887a0"},"cell_type":"code","source":"class Double_unit(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels, kernel_size=3, padding=1):#in_channels, middle_channels, out_channels, upscale=Upscale.upsample_bilinear):\n        super().__init__()\n        self.layer = nn.Sequential(\n            #nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_channels, middle_channels, kernel_size=kernel_size, padding=padding),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(middle_channels, out_channels, kernel_size=kernel_size, padding=padding),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        batch_size,C,H,W = x.shape\n        return self.layer(x)\nclass Standard_unit(nn.Module):\n    def __init__(self, stage, nb_filter, kernel_size=3, padding=1):#in_channels, middle_channels, out_channels, upscale=Upscale.upsample_bilinear):\n        super().__init__()\n        self.layer = nn.Sequential(\n            #nn.Upsample(scale_factor=2),\n            nn.Conv2d(nb_filter, nb_filter, kernel_size=kernel_size, padding=padding),\n            nn.BatchNorm2d(nb_filter),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(nb_filter, nb_filter, kernel_size=kernel_size, padding=padding),\n            nn.BatchNorm2d(nb_filter),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        batch_size,C,H,W = x.shape\n        return self.layer(x)\n\nclass Standard_unit1(nn.Module):\n    def __init__(self, stage, in_c, nb_filter, kernel_size=3, padding=1):#in_channels, middle_channels, out_channels, upscale=Upscale.upsample_bilinear):\n        super().__init__()\n        self.layer = nn.Sequential(\n            #nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_c, nb_filter, kernel_size=kernel_size, padding=padding),\n            nn.BatchNorm2d(nb_filter),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(nb_filter, nb_filter, kernel_size=kernel_size, padding=padding),\n            nn.BatchNorm2d(nb_filter),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        batch_size,C,H,W = x.shape\n        return self.layer(x)\n    \nclass Ups(nn.Module):\n    def __init__(self, in_chan, out_chan, upscale=1):#in_channels, middle_channels, out_channels, upscale=Upscale.upsample_bilinear):\n        super().__init__()\n        if upscale == 1:#Upscale.transposed_conv:\n            self.layer = nn.Sequential(\n                nn.ConvTranspose2d(in_chan, out_chan, 3, stride=2, padding=1, output_padding=1),\n                nn.BatchNorm2d(out_chan),\n                nn.ReLU(inplace=True)\n            )\n        elif upscale == 2:#Upscale.upsample_bilinear:\n            self.layer = nn.Upsample(scale_factor=2,mode='bilinear')\n            nn.Conv2d(in_chan, out_chan, kernel_size=1, padding=0),\n            nn.BatchNorm2d(out_chan),\n            nn.ReLU(inplace=True),\n        else:\n            self.layer = nn.PixelShuffle(upscale_factor=2)\n\n\n    def forward(self, x):\n        batch_size,C,H,W = x.shape\n        return self.layer(x)    \n\n\"\"\"\ndef standard_unit(input_tensor, stage, nb_filter, kernel_size=3):\n\n    x = Conv2D(nb_filter, (kernel_size, kernel_size), activation=act, name='conv'+stage+'_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(input_tensor)\n    x = Dropout(dropout_rate, name='dp'+stage+'_1')(x)\n    x = Conv2D(nb_filter, (kernel_size, kernel_size), activation=act, name='conv'+stage+'_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(x)\n    x = Dropout(dropout_rate, name='dp'+stage+'_2')(x)\n\n    return x\n\"\"\"\n\"\"\"\nStandard UNet++ [Zhou et.al, 2018]\nTotal params: 9,041,601\n\"\"\"\nclass UnetPP(nn.Module):\n    def __init__(self, num_class):#in_channels, middle_channels, out_channels, upscale=Upscale.upsample_bilinear):\n        super().__init__()\n        nb_filter = [32,64,128,256,512]\n        \n        self.conv1_1 = Standard_unit1(stage='11',in_c=3, nb_filter=nb_filter[0])\n        self.pool1 = nn.MaxPool2d(2, 2)\n\n        self.conv2_1 = Standard_unit1(stage='21',in_c=nb_filter[0], nb_filter=nb_filter[1])\n        self.pool2 = nn.MaxPool2d(2, 2)\n\n        self.up1_2 = Ups(nb_filter[1],nb_filter[0])#Conv2DTranspose(nb_filter[0])(conv2_1)\n        #self.conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)\n        self.conv1_2 = Standard_unit1(stage='12',in_c=nb_filter[0]+nb_filter[0], nb_filter=nb_filter[0])\n\n        self.conv3_1 = Standard_unit1( stage='31', in_c=nb_filter[1],nb_filter=nb_filter[2])\n        self.pool3 = nn.MaxPool2d(2, 2)\n\n        self.up2_2 = Ups(nb_filter[2],nb_filter[1])#Conv2DTranspose(nb_filter[1])(conv3_1)\n        #self.conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)\n        self.conv2_2 = Standard_unit1( stage='22', in_c=nb_filter[1]+nb_filter[1], nb_filter=nb_filter[1])\n\n        self.up1_3 = Ups(nb_filter[1],nb_filter[0])#Conv2DTranspose(nb_filter[0])(conv2_2)\n        #self.conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)\n        self.conv1_3 = Standard_unit1( stage='13', in_c=nb_filter[0]*3, nb_filter=nb_filter[0])\n\n        self.conv4_1 = Standard_unit1(stage='41',in_c=nb_filter[2], nb_filter=nb_filter[3])\n        self.pool4 = nn.MaxPool2d(2, 2)\n\n        self.up3_2 = Ups(nb_filter[3],nb_filter[2])#Conv2DTranspose(nb_filter[2])(conv4_1)\n        #self.conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)\n        self.conv3_2 = Standard_unit1(stage='32',in_c=nb_filter[2]+nb_filter[2], nb_filter=nb_filter[2])\n\n        self.up2_3 = Ups(nb_filter[2],nb_filter[1])#Conv2DTranspose(nb_filter[1])(conv3_2)\n        #self.conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)\n        self.conv2_3 = Standard_unit1( stage='23', in_c=nb_filter[1]*3,nb_filter=nb_filter[1])\n\n        self.up1_4 = Ups(nb_filter[1],nb_filter[0])#Conv2DTranspose(nb_filter[0])(conv2_3)\n        #self.conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)\n        self.conv1_4 = Standard_unit1(stage='14', in_c=nb_filter[0]*4,nb_filter=nb_filter[0])\n\n        self.conv5_1 = Standard_unit1( stage='51',in_c=nb_filter[3], nb_filter=nb_filter[4])\n\n        self.up4_2 = Ups(nb_filter[4],nb_filter[3])#Conv2DTranspose(nb_filter[3])(conv5_1)\n        #self.conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)\n        self.conv4_2 = Standard_unit1(stage='42', in_c=nb_filter[3]+nb_filter[3],nb_filter=nb_filter[3])\n\n        self.up3_3 = Ups(nb_filter[3],nb_filter[2])#Conv2DTranspose(nb_filter[2])(conv4_2)\n        #self.conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)\n        self.conv3_3 = Standard_unit1( stage='33', in_c=nb_filter[2]*3,nb_filter=nb_filter[2])\n\n        self.up2_4 = Ups(nb_filter[2],nb_filter[1])#Conv2DTranspose(nb_filter[1])(conv3_3)\n        #self.conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)\n        self.conv2_4 = Standard_unit1(stage='24',in_c=nb_filter[1]*4, nb_filter=nb_filter[1])\n\n        self.up1_5 = Ups(nb_filter[1],nb_filter[0])#Conv2DTranspose(nb_filter[0])(conv2_4)\n        #self.conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)\n        self.conv1_5 = Standard_unit1(stage='15', in_c=nb_filter[0]*5,nb_filter=nb_filter[0])        \n        \n        self.nestnet_output_1 = nn.Conv2d(nb_filter[0],num_class, 1, padding=0)#(conv1_2)\n        self.nestnet_output_2 = nn.Conv2d(nb_filter[0],num_class, 1, padding=0)#(conv1_3)\n        self.nestnet_output_3 = nn.Conv2d(nb_filter[0],num_class, 1, padding=0)#(conv1_4)\n        self.nestnet_output_4 = nn.Conv2d(nb_filter[0],num_class, 1, padding=0)#(conv1_5)        \n        \n        #= nn.Conv2d(num_class,num_class, 1, padding=0)#\n    def forward(self, img_input):#, img_rows, img_cols, color_type=1, num_class=1, deep_supervision=False):\n        img_input=img_input.float()\n        #nb_filter = [32,64,128,256,512]\n\n        # Handle Dimension Ordering for different backends\n        #global bn_axis\n        #if K.image_dim_ordering() == 'tf':\n        #  bn_axis = 3\n        #  img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n        #else:\n        #  bn_axis = 1\n        #  img_input = Input(shape=(color_type, img_rows, img_cols), name='main_input')\n        bn_axis = 1\n        \n        conv1_1 = self.conv1_1(img_input)\n        pool1 = self.pool1(conv1_1)\n\n        conv2_1 = self.conv2_1(pool1)\n        pool2 = self.pool2(conv2_1)\n\n        up1_2 = self.up1_2(conv2_1)\n        conv1_2 = torch.cat([up1_2, conv1_1],  bn_axis)\n        conv1_2 = self.conv1_2(conv1_2)\n\n        conv3_1 = self.conv3_1(pool2)\n        pool3 = self.pool3(conv3_1)\n\n        up2_2 = self.up2_2(conv3_1)\n        conv2_2 = torch.cat([up2_2, conv2_1],  bn_axis)\n        conv2_2 = self.conv2_2(conv2_2)\n\n        up1_3 = self.up1_3(conv2_2)\n        conv1_3 = torch.cat([up1_3, conv1_1, conv1_2],  bn_axis)\n        conv1_3 = self.conv1_3(conv1_3)\n\n        conv4_1 = self.conv4_1(pool3)\n        pool4 = self.pool4(conv4_1)\n\n        up3_2 = self.up3_2(conv4_1)\n        conv3_2 = torch.cat([up3_2, conv3_1],  bn_axis)\n        conv3_2 = self.conv3_2(conv3_2)\n\n        up2_3 = self.up2_3(conv3_2)\n        conv2_3 = torch.cat([up2_3, conv2_1, conv2_2],  bn_axis)\n        conv2_3 = self.conv2_3(conv2_3)\n\n        up1_4 = self.up1_4(conv2_3)\n        conv1_4 = torch.cat([up1_4, conv1_1, conv1_2, conv1_3], bn_axis)\n        conv1_4 = self.conv1_4(conv1_4)\n\n        conv5_1 = self.conv5_1(pool4)\n\n        up4_2 = self.up4_2(conv5_1)\n        conv4_2 = torch.cat([up4_2, conv4_1],  bn_axis)\n        conv4_2 = self.conv4_2(conv4_2)\n\n        up3_3 = self.up3_3(conv4_2)\n        conv3_3 = torch.cat([up3_3, conv3_1, conv3_2],  bn_axis)\n        conv3_3 = self.conv3_3(conv3_3)\n\n        up2_4 = self.up2_4(conv3_3)\n        conv2_4 = torch.cat([up2_4, conv2_1, conv2_2, conv2_3],  bn_axis)\n        conv2_4 = self.conv2_4(conv2_4)\n\n        up1_5 = self.up1_5(conv2_4)\n        conv1_5 = torch.cat([up1_5, conv1_1, conv1_2, conv1_3, conv1_4],  bn_axis)\n        conv1_5 = self.conv1_5(conv1_5)\n    \n        nestnet_output_1 = self.nestnet_output_1(conv1_2)\n        nestnet_output_2 = self.nestnet_output_2(conv1_3)\n        nestnet_output_3 = self.nestnet_output_3(conv1_4)\n        nestnet_output_4 = self.nestnet_output_4(conv1_5)\n        \"\"\"\n        if deep_supervision:\n            model = Model(input=img_input, output=[nestnet_output_1,\n                                                   nestnet_output_2,\n                                                   nestnet_output_3,\n                                                   nestnet_output_4])\n        else:\n            model = Model(input=img_input, output=[nestnet_output_4])\n        \"\"\"\n        return nestnet_output_4, nestnet_output_3, nestnet_output_2, nestnet_output_1\n\n    \n    \n    \n    \nclass LinkNet34deeps(nn.Module):\n    def __init__(self, num_classes, num_channels=3):\n        super().__init__()\n        assert num_channels == 3, \"num channels not used now. to use changle first conv layer to support num channels other then 3\"\n        filters = [64, 128, 256, 512]\n        resnet = models.resnet34(pretrained=True)\n\n        self.firstconv = resnet.conv1\n        self.firstbn = resnet.bn1\n        self.firstrelu = resnet.relu\n        self.firstmaxpool = resnet.maxpool\n        self.encoder1 = resnet.layer1\n        self.encoder2 = resnet.layer2\n        self.encoder3 = resnet.layer3\n        self.encoder4 = resnet.layer4\n\n        # Decoder\n        self.decoder4 = DecoderBlock(filters[3], filters[2])\n        self.conv4=nn.Sequential(\n            nn.Conv2d(512,256,1),\n            nn.BatchNorm2d(256),\n            nonlinearity(inplace=True),\n        ) #nn.Conv2d(512,256,1) \n        self.conv44f=nn.Conv2d(64,1,1)\n        self.conv44=nn.Sequential(\n            nn.Conv2d(256, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nonlinearity(inplace=True),\n            #nn.Conv2d(64, 1, 1),\n        ) \n        self.decoder3 = DecoderBlock(filters[2], filters[1])\n        self.conv3=nn.Sequential(\n            nn.Conv2d(256,128,1),\n            nn.BatchNorm2d(128),\n            nonlinearity(inplace=True),\n        ) #nn.Conv2d(256,128,1) \n        self.conv33f=nn.Conv2d(64,1,1)\n        self.conv33= nn.Sequential(\n            nn.Conv2d(128, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nonlinearity(inplace=True),\n            #nn.Conv2d(64, 1, 1),\n        ) \n        self.decoder2 = DecoderBlock(filters[1], filters[0])\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(128,64,1),\n            nn.BatchNorm2d(64),\n            nonlinearity(inplace=True),\n        ) #nn.Conv2d(128,64,1)  \n        self.conv22f=nn.Conv2d(64,1,1)\n        self.conv22=nn.Sequential(\n            nn.Conv2d(64, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nonlinearity(inplace=True),\n            #nn.Conv2d(64, 1, 1),\n        ) \n        self.decoder1 = DecoderBlock(filters[0], filters[0])\n\n        # Final Classifier\n        self.finaldeconv1 = ConvUp(filters[0], filters[0])\n        \n        # Final Classifier\n        self.logit = nn.Sequential(\n            nn.Conv2d(128, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nonlinearity(inplace=True),\n            nn.Conv2d(64, 1, 1),\n        )        \n        self.logit_image = nn.Sequential(\n            #nn.Linear(512, 128),\n            #nn.ReLU(inplace=True),\n            #nn.Linear(128, 1),\n            nn.Linear(64, 1),\n        )\n        self.fuse_image = nn.Sequential(\n            nn.Linear(512, 64),\n            #nn.ReLU(inplace=True),\n            #nn.Linear(128, 1),\n        )\n\n    # noinspection PyCallingNonCallable\n    def forward(self, x):\n        # Encoder\n        batch_size,dim,m,n=x.shape\n        x = x.float()\n        x = self.firstconv(x)\n        x = self.firstbn(x)\n        x = self.firstrelu(x)\n        #x = self.firstmaxpool(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        d4 = torch.cat([self.decoder4(e4) , e3], 1)#concat([self.decoder5(e5) , e4])\n        d4 = self.conv4(d4)\n        f4 = self.conv44(d4)\n        # d4 = e3\n        #d3 = self.decoder3(d4) + e2\n        #print(e2.shape)\n        d3 = torch.cat([self.decoder3(d4) , e2], 1)#concat([self.decoder5(e5) , e4])\n        #print(d3.shape)\n        d3 = self.conv3(d3)\n        f3 = self.conv33(d3)\n        \n        #d2 = self.decoder2(d3) + e1\n        d2 = torch.cat([self.decoder2(d3) , e1], 1)#concat([self.decoder5(e5) , e4])\n        d2 = self.conv2(d2)\n        f2 = self.conv22(d2)\n        \n        #d1 = self.decoder1(d2)\n\n        # Final Classification\n        f = self.finaldeconv1(d2)\n        #f = self.finalrelu1(f)\n        #f = self.logit(f)\n          \n        f21 = F.upsample(f2,scale_factor= 2, mode='bilinear',align_corners=False)\n        f22 = self.conv22f(f21)  \n        \n        f31 = F.upsample(f3,scale_factor= 4, mode='bilinear',align_corners=False)\n        f33 = self.conv33f(f31)  \n        \n        f41 = F.upsample(f4,scale_factor= 8, mode='bilinear',align_corners=False)\n        f44 = self.conv44f(f41)  \n\n        e = F.adaptive_avg_pool2d(e4, output_size=1).view(batch_size,-1) #image pool#-512-1-1\n        e = F.dropout(e, p=0.50, training=self.training)#\n        fuse_image  = self.fuse_image(e)#-64-1-1\n        logit_image = self.logit_image(fuse_image).view(-1)#-1-1-1\n\n        fuse_pixel=f+f21+f31+f41\n        \n        fuse = torch.cat([ #fuse\n            fuse_pixel,\n            F.upsample(fuse_image.view(batch_size,-1,1,1,),scale_factor=m, mode='nearest')\n        ],1)     \n        \n               \n        f = self.logit(fuse)   \n        \n        return f, f22, f33, f44, logit_image\n    \n        \"\"\"\n        self.fuse_pixel  = nn.Sequential(        \n            nn.Conv2d(320, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.logit_pixel  = nn.Sequential(\n            #nn.Conv2d(320, 64, kernel_size=3, padding=1),\n            #nn.ReLU(inplace=True),\n            nn.Conv2d( 64,  1, kernel_size=1, padding=0),\n        )\n\n        self.logit_image = nn.Sequential(\n            #nn.Linear(512, 128),\n            #nn.ReLU(inplace=True),\n            #nn.Linear(128, 1),\n            nn.Linear(64, 1),\n        )\n        self.fuse_image = nn.Sequential(\n            nn.Linear(512, 64),\n            #nn.ReLU(inplace=True),\n            #nn.Linear(128, 1),\n        )\n        self.fuse = nn.Sequential(\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.logit = nn.Sequential(\n            #nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            #nn.ReLU(inplace=True),\n            nn.Conv2d( 64,  1, kernel_size=1, padding=0),\n        )    \n        \"\"\"\n    \n    \n    \ndef criterion(output_4, output_3, output_2, output_1, logit_image, truth_pixel, is_average=True):\n    #print(torch.sum(torch.sum(truth_pixel)))\n    bat,dim,m,n=truth_pixel.shape\n\n    truth_image = torch.Tensor(bat)\n    i=0\n    for x in truth_pixel:\n        truth_image[i]=(x.squeeze().sum()>0).cuda().float()\n        i=i+1\n\n    truth_image=truth_image.type(torch.cuda.FloatTensor)\n\n    loss_image = F.binary_cross_entropy_with_logits(logit_image, truth_image, reduce=is_average)#1-1-1\n    \n    loss_logit1 = F.binary_cross_entropy_with_logits(output_1, truth_pixel)#1-1-1\n    loss_logit2 = F.binary_cross_entropy_with_logits(output_2, truth_pixel)#1-1-1\n    loss_logit3 = F.binary_cross_entropy_with_logits(output_3, truth_pixel)#1-1-1\n    loss_logit4 = F.binary_cross_entropy_with_logits(output_4, truth_pixel)#1-1-1\n\n    #weight_image, weight_pixel = 0.1, 10  #focal\n    weight_1, weight_2, weight_3, weight_4, weight_image = 0.2, 0.2, 0.2, 1, 0.05  #lovasz?\n    #weight_image, weight_pixel = 0.1, 2 #bce\n\n\n\n    return weight_1*loss_logit1+ weight_2*loss_logit2+weight_3*loss_logit3+weight_4*loss_logit4+weight_image*loss_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b303402ee756bff5bda98bad959e82f0a059f072"},"cell_type":"code","source":"model = LinkNet34deeps(1) #","execution_count":null,"outputs":[]},{"metadata":{"id":"drM82uSCseyh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":6875},"outputId":"98ddfcb7-df3b-4777-d501-3834ec8c226d","executionInfo":{"status":"ok","timestamp":1537773894980,"user_tz":-480,"elapsed":1205,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"a7e22773e7d2b994e4f42a2aba95425a1aed198a"},"cell_type":"code","source":"#print(model)\nparams = list(model.parameters())\nk = 0\nfor i in params:\n    l = 1\n    print(\"该层的结构：\" + str(list(i.size())))\n    for j in i.size():\n        l *= j\n    print(\"该层参数和：\" + str(l))\n    k = k + l\nprint(\"总参数数量和：\" + str(k))","execution_count":null,"outputs":[]},{"metadata":{"id":"xYcMcdtZseyl","colab_type":"code","colab":{},"trusted":true,"_uuid":"3e84d49cd0d11d7ff60c6445d2f14efa7f5e0dad"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"xScLKLTEseyn","colab_type":"text","_uuid":"6e4da0d022a4daee73e4df9cbffe4fa1b04ef671"},"cell_type":"markdown","source":"## Loss function\nDue to non-differentiability of the competition evaluation metric (F2 Score at different intersection over union (IoU) thresholds) we use combination of  the BinnaryCrossEntropy loss and minus Jaccard index (https://en.wikipedia.org/wiki/Jaccard_index)\n"},{"metadata":{"id":"XbPTaI9sseyo","colab_type":"code","colab":{},"trusted":true,"_uuid":"ab7e8de4cf4f9b1e641b8823332220aaba6f5f36"},"cell_type":"code","source":"   \"\"\"\n    def set_loss(self):\n        if self.activation_func == 'softmax':\n            loss_function = partial(mixed_dice_cross_entropy_loss,\n                                    dice_loss=multiclass_dice_loss,\n                                    cross_entropy_loss=nn.CrossEntropyLoss(),\n                                    dice_activation='softmax',\n                                    dice_weight=self.architecture_config['model_params']['dice_weight'],\n                                    cross_entropy_weight=self.architecture_config['model_params']['bce_weight']\n                                    )\n        elif self.activation_func == 'sigmoid':\n            loss_function = partial(mixed_dice_bce_loss,\n                                    dice_loss=multiclass_dice_loss,\n                                    bce_loss=nn.BCEWithLogitsLoss(),\n                                    dice_activation='sigmoid',\n                                    dice_weight=self.architecture_config['model_params']['dice_weight'],\n                                    bce_weight=self.architecture_config['model_params']['bce_weight']\n                                    )\n        else:\n            raise Exception('Only softmax and sigmoid activations are allowed')\n        self.loss_function = [('mask', loss_function, 1.0)]\n\"\"\"\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=0, eps=1e-7):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n        self.eps = eps\n\n    def forward(self, output, target):\n        return 1 - (2 * torch.sum(output * target) + self.smooth) / (\n                torch.sum(output) + torch.sum(target) + self.smooth + self.eps)\n\n\ndef mixed_dice_bce_loss(output, target, dice_weight=0.2, dice_loss=None,\n                        bce_weight=0.9, bce_loss=None,\n                        smooth=0, dice_activation='sigmoid'):\n    num_classes = output.size(1)\n    target = target[:, :num_classes, :, :].long()\n    if bce_loss is None:\n        bce_loss = nn.BCEWithLogitsLoss()\n    if dice_loss is None:\n        dice_loss = multiclass_dice_loss\n    return dice_weight * dice_loss(output, target, smooth, dice_activation) + bce_weight * bce_loss(output, target)\n\n\ndef mixed_dice_cross_entropy_loss(output, target, dice_weight=0.5, dice_loss=None,\n                                  cross_entropy_weight=0.5, cross_entropy_loss=None, smooth=0,\n                                  dice_activation='softmax'):\n    num_classes_without_background = output.size(1) - 1\n    dice_output = output[:, 1:, :, :]\n    dice_target = target[:, :num_classes_without_background, :, :].long()\n    cross_entropy_target = torch.zeros_like(target[:, 0, :, :]).long()\n    for class_nr in range(num_classes_without_background):\n        cross_entropy_target = where(target[:, class_nr, :, :], class_nr + 1, cross_entropy_target)\n    if cross_entropy_loss is None:\n        cross_entropy_loss = nn.CrossEntropyLoss()\n    if dice_loss is None:\n        dice_loss = multiclass_dice_loss\n    return dice_weight * dice_loss(dice_output, dice_target, smooth,\n                                   dice_activation) + cross_entropy_weight * cross_entropy_loss(output,\n                                                                                                cross_entropy_target)\n\n\ndef multiclass_dice_loss(output, target, smooth=0, activation='softmax'):\n    \"\"\"Calculate Dice Loss for multiple class output.\n    Args:\n        output (torch.Tensor): Model output of shape (N x C x H x W).\n        target (torch.Tensor): Target of shape (N x H x W).\n        smooth (float, optional): Smoothing factor. Defaults to 0.\n        activation (string, optional): Name of the activation function, softmax or sigmoid. Defaults to 'softmax'.\n    Returns:\n        torch.Tensor: Loss value.\n    \"\"\"\n    if activation == 'softmax':\n        activation_nn = torch.nn.Softmax2d()\n    elif activation == 'sigmoid':\n        activation_nn = torch.nn.Sigmoid()\n    else:\n        raise NotImplementedError('only sigmoid and softmax are implemented')\n\n    loss = 0\n    dice = DiceLoss(smooth=smooth)\n    output = activation_nn(output)\n    num_classes = output.size(1)\n    target.data = target.data.float()\n    for class_nr in range(num_classes):\n        loss += dice(output[:, class_nr, :, :], target[:, class_nr, :, :])\n    return loss / num_classes\n\n\ndef where(cond, x_1, x_2):\n    cond = cond.long()\n    return (cond * x_1) + ((1 - cond) * x_2)","execution_count":null,"outputs":[]},{"metadata":{"id":"CSAY4N_Qseyq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2557},"outputId":"cc22921a-822f-422b-930f-f0c079b8282a","executionInfo":{"status":"ok","timestamp":1537773900348,"user_tz":-480,"elapsed":2703,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"4a4f7ef34c402ee0f91204e932c18e285027a862"},"cell_type":"code","source":"from torchvision import models\nimport torchvision\nres34=torchvision.models.resnet34(pretrained=True)\nprint(res34)","execution_count":null,"outputs":[]},{"metadata":{"id":"fxqxyPN-seyv","colab_type":"code","colab":{},"trusted":true,"_uuid":"ecf291b00c02b05d50aa57b637cbf2b6fefb98d8"},"cell_type":"code","source":" class LossBinary:\n    \"\"\"\n     Implementation from  https://github.com/ternaus/robot-surgery-segmentation\n    \"\"\"\n\n    def __init__(self, jaccard_weight=0):\n        self.nll_loss = nn.BCEWithLogitsLoss()\n        self.jaccard_weight = jaccard_weight\n\n    def __call__(self, outputs, targets):\n        loss = self.nll_loss(outputs, targets)\n\n        if self.jaccard_weight:\n            eps = 1e-15\n            jaccard_target = (targets == 1.0).float()\n            jaccard_output = F.sigmoid(outputs)\n\n            intersection = (jaccard_output * jaccard_target).sum()\n            union = jaccard_output.sum() + jaccard_target.sum()\n\n            loss -= self.jaccard_weight * torch.log((intersection + eps) / (union - intersection + eps))\n        return loss\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Jx3XUzx6seyy","colab_type":"text","_uuid":"b51217e9b8b522f967546393856fd93413eff033"},"cell_type":"markdown","source":"## Validation routine"},{"metadata":{"id":"Nud1udgBseyz","colab_type":"code","colab":{},"trusted":true,"_uuid":"bc72edd37f4081fd5a9a16b13cb51167573405eb"},"cell_type":"code","source":"def validation(model: nn.Module, criterion, valid_loader):\n    print(\"Validation on hold-out....\")\n    model.eval()\n    losses = []\n    jaccard = []\n    ious = []\n    for inputs, targets in valid_loader:\n        inputs = variable(inputs, volatile=True)\n        targets = variable(targets)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        losses.append(loss.data[0])\n        jaccard += [get_jaccard(targets, (outputs > 0).float()).data[0]]\n        ious += [get_iou_vector(targets, (outputs > 0).float())]\n    \n    valid_ious = np.mean(ious)\n    \n    valid_loss = np.mean(losses)  # type: float\n\n    valid_jaccard = np.mean(jaccard)\n\n    print('Valid loss: {:.5f}, jaccard: {:.5f}, ious: {:.5f}'.format(valid_loss, valid_jaccard, valid_ious))\n    metrics = {'valid_loss': valid_loss, 'jaccard_loss': valid_jaccard, 'ious_loss': valid_ious}\n    return metrics\ndef validation4(model: nn.Module, criterion, valid_loader):\n    print(\"Validation on hold-out....\")\n    model.eval()\n    losses = []\n    jaccard = []\n    ious = []\n    for inputs, targets in valid_loader:\n        inputs = variable(inputs, volatile=True)\n        targets = variable(targets)\n        outputs,o2,o3,o4= model(inputs)\n        loss = criterion(outputs,o2,o3,o4, targets)\n        losses.append(loss.data[0])\n        jaccard += [get_jaccard(targets, (outputs > 0).float()).data[0]]\n        ious += [get_iou_vector(targets, (outputs > 0).float())]\n    \n    valid_ious = np.mean(ious)\n    \n    valid_loss = np.mean(losses)  # type: float\n\n    valid_jaccard = np.mean(jaccard)\n\n    print('Valid loss: {:.5f}, jaccard: {:.5f}, ious: {:.5f}'.format(valid_loss, valid_jaccard, valid_ious))\n    metrics = {'valid_loss': valid_loss, 'jaccard_loss': valid_jaccard, 'ious_loss': valid_ious}\n    return metrics\ndef validation5(model: nn.Module, criterion, valid_loader):\n    print(\"Validation on hold-out....\")\n    model.eval()\n    losses = []\n    jaccard = []\n    ious = []\n    for inputs, targets in valid_loader:\n        inputs = variable(inputs, volatile=True)\n        targets = variable(targets)\n        outputs,o2,o3,o4,o5= model(inputs)\n        loss = criterion(outputs,o2,o3,o4,o5, targets)\n        losses.append(loss.data[0])\n        jaccard += [get_jaccard(targets, (outputs > 0).float()).data[0]]\n        ious += [get_iou_vector(targets, (outputs > 0).float())]\n    \n    valid_ious = np.mean(ious)\n    \n    valid_loss = np.mean(losses)  # type: float\n\n    valid_jaccard = np.mean(jaccard)\n\n    print('Valid loss: {:.5f}, jaccard: {:.5f}, ious: {:.5f}'.format(valid_loss, valid_jaccard, valid_ious))\n    metrics = {'valid_loss': valid_loss, 'jaccard_loss': valid_jaccard, 'ious_loss': valid_ious}\n    return metrics\ndef get_jaccard(y_true, y_pred):\n    epsilon = 1e-15\n    intersection = (y_pred * y_true).sum(dim=-2).sum(dim=-1).sum(dim = -1)\n    union = y_true.sum(dim=-2).sum(dim=-1).sum(dim=-1) + y_pred.sum(dim=-2).sum(dim=-1).sum(dim = -1)\n\n    return (intersection / (union - intersection + epsilon)).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa356805b4e0893c3f3cf8fb5c69ca6c93db1be3"},"cell_type":"code","source":"import numpy as np # linear algebra\n\ndef get_iou_vector_single(A, B):\n    #A:True,B:Pred\n    #print(A.shape)\n    batch_size = A.shape[0]\n    print(batch_size)\n    #metric = []\n    #for batch in range(batch_size):\n    t, p = A.squeeze(), B.squeeze()\n    if np.count_nonzero(t) == 0 and np.count_nonzero(p) > 0:\n        metric=0\n        return np.mean(metric)\n        #continue\n    if np.count_nonzero(t) == 0 and np.count_nonzero(p) == 0:\n        metric=1\n        return np.mean(metric)\n        #continue\n\n    iou = jaccard(t, p)\n    #print(iou)\n    thresholds = np.arange(0.5, 1, 0.05)\n    s = []\n    for thresh in thresholds:\n        #s.append(iou > thresh)\n        s.append(iou > thresh)\n    metric=np.mean(s)\n    #print(metric.shape)\n    return np.mean(metric)\n\ndef get_iou_vector(A, B):\n    #A:True,B:Pred\n    #print(A.shape)\n    batch_size = A.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        t, p = A[batch].squeeze(), B[batch].squeeze()\n        if np.count_nonzero(t) == 0 and np.count_nonzero(p) > 0:\n            metric.append(0)\n            continue\n        if np.count_nonzero(t) == 0 and np.count_nonzero(p) == 0:\n            metric.append(1)\n            continue\n\n        iou = jaccard(t, p)\n        #print(iou)\n        thresholds = np.arange(0.5, 1, 0.05)\n        s = []\n        for thresh in thresholds:\n            #s.append(iou > thresh)\n            s.append(iou > thresh)\n        metric.append(np.mean(s))\n    #print(metric.shape)\n    return np.mean(metric)\n\n\n\ndef jaccard(y_true, y_pred):\n    epsilon = 1e-15\n    intersection = (y_pred * y_true).sum().sum()\n    union = y_true.sum().sum() + y_pred.sum().sum()\n\n    return ((intersection + epsilon)/ (union - intersection + epsilon)).mean()\n\n#thresholds = np.linspace(0, 1, 50)\n#ious = np.array([get_iou_vector(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"id":"9aF4uW9Usey4","colab_type":"code","colab":{},"trusted":true,"_uuid":"b0d64f75513b27784e65d3f0a95627a2a13ee24e"},"cell_type":"code","source":"class MyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return super(MyEncoder, self).default(obj)\n\n#json_1 = {'num':1112, 'date':datetime.now()}\n#print(json.dumps(json_1, cls=MyEncoder))","execution_count":null,"outputs":[]},{"metadata":{"id":"EdX--GSpsey6","colab_type":"code","colab":{},"trusted":true,"_uuid":"b84381be56da1e5fb27afa8c50bbba8db80b8fe6"},"cell_type":"code","source":"# sume helper functions\ndef variable(x, volatile=False):\n    if isinstance(x, (list, tuple)):\n        return [variable(y, volatile=volatile) for y in x]\n    return cuda(Variable(x, volatile=volatile))\n\ndef cuda(x):\n    return x.cuda(async=True) if torch.cuda.is_available() else x\n\ndef write_event(log, lr, step: int, **data):\n    data['lr'] = lr\n    data['step'] = step\n    data['dt'] = datetime.now().isoformat()\n    log.write(json.dumps(data, sort_keys=True, cls=MyEncoder))\n    log.write('\\n')\n    log.flush()","execution_count":null,"outputs":[]},{"metadata":{"id":"-PCE7Dddsey9","colab_type":"text","_uuid":"63546ccac10440b1aec930d2d4ffcf450c0b1e4e"},"cell_type":"markdown","source":"# ****LOSS********"},{"metadata":{"id":"4-qC5pfvsey-","colab_type":"code","colab":{},"trusted":true,"_uuid":"952a46728dcd248e99addfd868baef1258f8eb53"},"cell_type":"code","source":"from __future__ import print_function, division\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\ntry:\n    from itertools import  ifilterfalse\nexcept ImportError: # py3k\n    from itertools import  filterfalse\n\n\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1: # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    \"\"\"\n    IoU for foreground class\n    binary: 1 foreground, 0 background\n    \"\"\"\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / union\n        ious.append(iou)\n    iou = mean(ious)    # mean accross images if per_image\n    return 100 * iou\n\n\ndef iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    \"\"\"\n    Array of IoU for each (non ignored) class\n    \"\"\"\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []    \n        for i in range(C):\n            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / union)\n        ious.append(iou)\n    ious = map(mean, zip(*ious)) # mean accross images if per_image\n    return 100 * np.array(ious)\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=False, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * signs)#Variable(signs))############\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    #loss = torch.dot(F.elu(errors_sorted)+1,  grad)#Variable(grad))#ELU+1\n    loss = torch.dot(F.relu(errors_sorted),  grad)#Variable(grad))#RELU\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n\nclass StableBCELoss(torch.nn.modules.Module):\n    def __init__(self):\n         super(StableBCELoss, self).__init__()\n    def forward(self, input, target):\n         neg_abs = - input.abs()\n         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n         return loss.mean()\n\n\ndef binary_xloss(logits, labels, ignore=None):\n    \"\"\"\n    Binary Cross entropy loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      ignore: void class id\n    \"\"\"\n    logits, labels = flatten_binary_scores(logits, labels, ignore)\n    loss = StableBCELoss()(logits, Variable(labels.float()))\n    return loss\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, only_present=False, per_image=False, ignore=None):\n    \"\"\"\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    \"\"\"\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), only_present=only_present)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), only_present=only_present)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, only_present=False):\n    \"\"\"\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n    \"\"\"\n    C = probas.size(1)\n    losses = []\n    for c in range(C):\n        fg = (labels == c).float() # foreground for class c\n        if only_present and fg.sum() == 0:\n            continue\n        errors = (fg - probas[:, c]).abs()#(Variable(fg) - probas[:, c]).abs()#############################\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, lovasz_grad(fg_sorted)))#Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)\n\n\ndef flatten_probas(probas, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch\n    \"\"\"\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = (labels != ignore)\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels\n\ndef xloss(logits, labels, ignore=None):\n    \"\"\"\n    Cross entropy loss\n    \"\"\"\n    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\n\ndef mean(l, ignore_nan=False, empty=0):\n    \"\"\"\n    nanmean compatible with generators.\n    \"\"\"\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(np.isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == 'raise':\n            raise ValueError('Empty mean')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n","execution_count":null,"outputs":[]},{"metadata":{"id":"CfC17KX3sezB","colab_type":"code","colab":{},"trusted":true,"_uuid":"2dc5d8396bbcf0adb7e74cf85b9f2cc663c71bec"},"cell_type":"code","source":"def lovasz_loss(outputs,targets):\n    return lovasz_hinge(outputs,targets)\n#lovasz_softmax(torch.from_numpy(outputs.data.numpy()),torch.from_numpy(targets.data.numpy()))\n\nclass LossBinary1:\n    \"\"\"\n     Implementation from  https://github.com/ternaus/robot-surgery-segmentation\n    \"\"\"\n\n    def __init__(self, jaccard_weight=0):\n        self.nll_loss = lovasz_loss#mixed_dice_bce_loss#lovasz_loss#lovasz_softmax#nn.BCEWithLogitsLoss()\n        self.jaccard_weight = jaccard_weight\n\n    def __call__(self, outputs, targets):\n        #print(outputs.shape)\n        #print(targets.shape)\n        loss = self.nll_loss(outputs, targets)\n\n        if self.jaccard_weight:\n            eps = 1e-15\n            jaccard_target = (targets == 1.0).float()\n            jaccard_output = F.sigmoid(outputs)\n\n            intersection = (jaccard_output * jaccard_target).sum()\n            union = jaccard_output.sum() + jaccard_target.sum()\n\n            loss -= self.jaccard_weight * torch.log((intersection + eps) / (union - intersection + eps))\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"830f2a22f927b65e9f26919ecce006648268b015"},"cell_type":"code","source":"def lovasz_hingeElu(logits, labels, per_image=False, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        loss = mean(lovasz_hinge_flatElu(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flatElu(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flatElu(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * signs)#Variable(signs))############\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    loss = torch.dot(F.elu(errors_sorted)+1,  grad)#Variable(grad))#ELU+1\n    #loss = torch.dot(F.relu(errors_sorted),  grad)#Variable(grad))#RELU\n    return loss\n\nclass LossBinaryElu:\n    \"\"\"\n     Implementation from  https://github.com/ternaus/robot-surgery-segmentation\n    \"\"\"\n\n    def __init__(self, jaccard_weight=0):\n        self.nll_loss = lovasz_hingeElu#lovasz_loss#mixed_dice_bce_loss#lovasz_loss#lovasz_softmax#nn.BCEWithLogitsLoss()\n        self.jaccard_weight = jaccard_weight\n\n    def __call__(self, outputs, targets):\n        #print(outputs.shape)\n        #print(targets.shape)\n        loss = self.nll_loss(outputs, targets)\n\n        if self.jaccard_weight:\n            eps = 1e-15\n            jaccard_target = (targets == 1.0).float()\n            jaccard_output = F.sigmoid(outputs)\n\n            intersection = (jaccard_output * jaccard_target).sum()\n            union = jaccard_output.sum() + jaccard_target.sum()\n\n            loss -= self.jaccard_weight * torch.log((intersection + eps) / (union - intersection + eps))\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"id":"Ule7etUXsezD","colab_type":"code","colab":{},"trusted":true,"_uuid":"c1de22b4a74bc7509c067bbe8d1bd06641849550"},"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, gamma):\n        super().__init__()\n        self.gamma = gamma\n        \n    def forward(self, input, target):\n        # Inspired by the implementation of binary_cross_entropy_with_logits\n        if not (target.size() == input.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n\n        max_val = (-input).clamp(min=0)\n        loss = input - input * target + max_val + ((-max_val).exp() + (-input - max_val).exp()).log()\n\n        # This formula gives us the log sigmoid of 1-p if y is 0 and of p if y is 1\n        invprobs = F.logsigmoid(-input * (target * 2 - 1))\n        loss = (invprobs * self.gamma).exp() * loss\n        \n        return loss.mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"sCQmNPrwsezG","colab_type":"text","_uuid":"5e50d63c79c5d2c2f89461650230930f2a454657"},"cell_type":"markdown","source":"# ****Train****"},{"metadata":{"id":"1YomIB6xsezI","colab_type":"code","colab":{},"trusted":true,"_uuid":"028608f611fbc50e1d2ef143318c67f1a2b1f2de"},"cell_type":"code","source":"\n# 'Cyclical Learning Rates for Training Neural Networks'- Leslie N. Smith, arxiv 2017\n#       https://arxiv.org/abs/1506.01186\n#       https://github.com/bckenstler/CLR\n\nclass CyclicLR():\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step = step\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: (0.5)**(x-1)\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step != None:\n            self.step = new_step\n        self.clr_iterations = 0.\n\n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step))\n        x = np.abs(self.clr_iterations/self.step - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n\n    def get_rate(self, epoch=None, num_epoches=None):\n\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        lr = self.clr()\n\n        return lr\n\n    def __str__(self):\n        string = 'Cyclical Learning Rates\\n' \\\n                + 'base_lr=%0.3f, max_lr=%0.3f'%(self.base_lr, self.max_lr)\n        return string\n\n\n\n# net ------------------------------------\n# https://github.com/pytorch/examples/blob/master/imagenet/main.py ###############\ndef adjust_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\ndef get_learning_rate(optimizer):\n    lr=[]\n    for param_group in optimizer.param_groups:\n       lr +=[ param_group['lr'] ]\n\n    assert(len(lr)==1) #we support only one param_group\n    lr = lr[0]\n\n    return lr\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"hJLNRIXjsezK","colab_type":"code","colab":{},"trusted":true,"_uuid":"0a992e43b0a4a699dffce461b30d1c2f3d18411c"},"cell_type":"code","source":"def adjust_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\ndef get_learning_rate(optimizer):\n    lr=[]\n    for param_group in optimizer.param_groups:\n       lr +=[ param_group['lr'] ]\n\n    assert(len(lr)==1) #we support only one param_group\n    lr = lr[0]\n\n    return lr","execution_count":null,"outputs":[]},{"metadata":{"id":"vrAzf8NfsezN","colab_type":"code","colab":{},"trusted":true,"_uuid":"d265e0f1eaa9af2e03abb4f37434218d184d43ac"},"cell_type":"code","source":"def save_checkpoint(state, is_best, filename='model_1.pt'):\n    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n    if is_best:\n        print (\"=> Saving a new best\")\n        torch.save(state, filename) # save checkpoint\n    else:\n        print (\"=> Validation Accuracy did not improve\")\n\n\"\"\"# Training the Model\nfor epoch in range(num_epochs):\n train(...) # Train\n acc = eval(...) # Evaluate after every epoch\n\n# Some stuff with acc(accuracy)\n ...\n\n# Get bool not ByteTensor\n is_best = bool(acc.numpy() > best_accuracy.numpy())\n # Get greater Tensor to keep track best acc\n best_accuracy = torch.FloatTensor(max(acc.numpy(), best_accuracy.numpy()))\n # Save checkpoint if is a new best\n save_checkpoint({\n 'epoch': start_epoch + epoch + 1,\n 'state_dict': model.state_dict(),\n 'best_accuracy': best_accuracy\n }, is_best,model_path)\n\"\"\"\n\n# main train routine\n# Implementation from  https://github.com/ternaus/robot-surgery-segmentation\ndef train(lr, model, criterion, train_loader, valid_loader, validation, init_optimizer, n_epochs=1, fold=1):\n    optimizer = init_optimizer(lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.5,patience=20)################\n    #model = nn.DataParallel(model, device_ids=None)\n    if torch.cuda.is_available():\n        model.cuda()\n    isrestore = False   \n    startepoch = 0\n    model_path = Path('model_{fold}.pt'.format(fold=fold))\n    if model_path.exists():\n        state = torch.load(str(model_path))\n        epoch = state['epoch']\n        step = state['step']\n        model.load_state_dict(state['model'])\n        print('Restored model, epoch {}, step {:,}'.format(epoch, step))\n        isrestore = True\n        startepoch = epoch\n        \n        valid_metrics = validation(model, criterion, valid_loader)\n        valid_loss = valid_metrics['valid_loss']\n        ious_loss = valid_metrics['ious_loss']\n        print(ious_loss)\n        best_accuracy=-ious_loss\n    else:\n        epoch = 1\n        step = 0\n\n    save = lambda ep: torch.save({\n        'model': model.state_dict(),\n        'epoch': ep,\n        'step': step,\n    }, str(model_path))\n    \n\n    report_each = 50\n    log = open('train_{fold}.log'.format(fold=fold),'at', encoding='utf8')\n    valid_losses = []\n    for epoch in range(epoch, n_epochs + 1):\n        model.train()\n        random.seed()\n        tq = tqdm(total=len(train_loader) *  BATCH_SIZE)\n        tq.set_description('Epoch {}, lr {}'.format(epoch, lr))\n        losses = []\n        tl = train_loader\n        try:\n            mean_loss = 0\n            for i, (inputs, targets) in enumerate(tl):\n                inputs, targets = variable(inputs), variable(targets)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                optimizer.zero_grad()\n                batch_size = inputs.size(0)\n                loss.backward()\n                optimizer.step()\n                step += 1\n                tq.update(batch_size)\n                losses.append(loss.data[0])\n                mean_loss = np.mean(losses[-report_each:])\n                tq.set_postfix(loss='{:.5f}'.format(mean_loss))\n                if i and i % report_each == 0:\n                    write_event(log, lr, step, loss=mean_loss)\n            write_event(log, lr, step, loss=mean_loss)########################lr\n            tq.close()\n            #save(epoch + 1)\n            valid_metrics = validation(model, criterion, valid_loader)\n            write_event(log, lr, step, **valid_metrics)#############################\n            valid_loss = valid_metrics['valid_loss']\n\n            acc= -valid_metrics['ious_loss']\n            scheduler.step(acc)###########################\n            lr=get_learning_rate(optimizer)###############################\n            \n            if epoch==1:\n                best_accuracy=-valid_metrics['ious_loss']#valid_loss#(valid_loss.from_numpy())\n                is_best=True\n                if is_best:\n                    print (\"=> Saving a new best\")\n                    save(epoch) # save checkpoint\n                else:\n                    print (\"=> Validation Accuracy did not improve\")\n            else:\n                #if isrestore == False:\n                is_best = bool(acc < best_accuracy)#(acc.numpy() > best_accuracy.numpy())\n                best_accuracy = min(acc, best_accuracy)#torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n                if is_best:\n                    print (\"=> Saving a new best\")\n                    save(epoch) # save checkpoint\n                else:\n                    print (\"=> Validation Accuracy did not improve\")\n                #else:\n                      \n                    \"\"\"if epoch == startepoch:\n                        best_accuracy=valid_loss\n                    else:\n                        is_best = bool(acc < best_accuracy)#(acc.numpy() > best_accuracy.numpy())\n                        best_accuracy = min(acc, best_accuracy)#torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n                        if is_best:\n                            print (\"=> Saving a new best\")\n                            save(epoch) # save checkpoint\n                        else:\n                            print (\"=> Validation Accuracy did not improve\")\"\"\"\n            valid_losses.append(valid_loss)\n        except KeyboardInterrupt:\n            tq.close()\n            print('Ctrl+C, saving snapshot')\n            save(epoch)\n            print('done.')\n            return\n        \n        \ndef train4(lr, model,train_loader, valid_loader,criterion, validation, init_optimizer, n_epochs=1, fold=1):\n    optimizer = init_optimizer(lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.5,patience=20)################\n    #model = nn.DataParallel(model, device_ids=None)\n    if torch.cuda.is_available():\n        model.cuda()\n    isrestore = False   \n    startepoch = 0\n    model_path = Path('model_{fold}.pt'.format(fold=fold))\n    if model_path.exists():\n        state = torch.load(str(model_path))\n        epoch = state['epoch']\n        step = state['step']\n        model.load_state_dict(state['model'])\n        print('Restored model, epoch {}, step {:,}'.format(epoch, step))\n        isrestore = True\n        startepoch = epoch\n        \n        valid_metrics = validation(model, criterion, valid_loader)\n        valid_loss = valid_metrics['valid_loss']\n        ious_loss = valid_metrics['ious_loss']\n        print(ious_loss)\n        best_accuracy=-ious_loss\n    else:\n        epoch = 1\n        step = 0\n\n    save = lambda ep: torch.save({\n        'model': model.state_dict(),\n        'epoch': ep,\n        'step': step,\n    }, str(model_path))\n    \n\n    report_each = 50\n    log = open('train_{fold}.log'.format(fold=fold),'at', encoding='utf8')\n    valid_losses = []\n    for epoch in range(epoch, n_epochs + 1):\n        model.train()\n        random.seed()\n        tq = tqdm(total=len(train_loader) *  BATCH_SIZE)\n        tq.set_description('Epoch {}, lr {}'.format(epoch, lr))\n        losses = []\n        tl = train_loader\n        try:\n            mean_loss = 0\n            for i, (inputs, targets) in enumerate(tl):\n                inputs, targets = variable(inputs), variable(targets)\n                outputs,o2,o3,o4 = model(inputs)\n                loss = criterion(outputs, o2, o3,o4, targets)\n                optimizer.zero_grad()\n                batch_size = inputs.size(0)\n                loss.backward()\n                optimizer.step()\n                step += 1\n                tq.update(batch_size)\n                losses.append(loss.data[0])\n                mean_loss = np.mean(losses[-report_each:])\n                tq.set_postfix(loss='{:.5f}'.format(mean_loss))\n                if i and i % report_each == 0:\n                    write_event(log, lr, step, loss=mean_loss)\n            write_event(log, lr, step, loss=mean_loss)########################lr\n            tq.close()\n            #save(epoch + 1)\n            valid_metrics = validation(model, criterion, valid_loader)\n            write_event(log, lr, step, **valid_metrics)#############################\n            valid_loss = valid_metrics['valid_loss']\n\n            acc= -valid_metrics['ious_loss']\n            #scheduler.step(acc)###########################\n            #lr=get_learning_rate(optimizer)###############################\n            \n            if epoch==1:\n                best_accuracy=-valid_metrics['ious_loss']#valid_loss#(valid_loss.from_numpy())\n                is_best=True\n                if is_best:\n                    print (\"=> Saving a new best\")\n                    save(epoch) # save checkpoint\n                else:\n                    print (\"=> Validation Accuracy did not improve\")\n            else:\n                #if isrestore == False:\n                is_best = bool(acc < best_accuracy)#(acc.numpy() > best_accuracy.numpy())\n                best_accuracy = min(acc, best_accuracy)#torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n                if is_best:\n                    print (\"=> Saving a new best\")\n                    save(epoch) # save checkpoint\n                else:\n                    print (\"=> Validation Accuracy did not improve\")\n                #else:\n                      \n                    \"\"\"if epoch == startepoch:\n                        best_accuracy=valid_loss\n                    else:\n                        is_best = bool(acc < best_accuracy)#(acc.numpy() > best_accuracy.numpy())\n                        best_accuracy = min(acc, best_accuracy)#torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n                        if is_best:\n                            print (\"=> Saving a new best\")\n                            save(epoch) # save checkpoint\n                        else:\n                            print (\"=> Validation Accuracy did not improve\")\"\"\"\n            valid_losses.append(valid_loss)\n        except KeyboardInterrupt:\n            tq.close()\n            print('Ctrl+C, saving snapshot')\n            save(epoch)\n            print('done.')\n            return\n        \ndef train5(lr, model,train_loader, valid_loader,criterion, validation, init_optimizer, n_epochs=1, fold=1):\n    optimizer = init_optimizer(lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.5,patience=20)################\n    #model = nn.DataParallel(model, device_ids=None)\n    if torch.cuda.is_available():\n        model.cuda()\n    isrestore = False   \n    startepoch = 0\n    model_path = Path('model_{fold}.pt'.format(fold=fold))\n    if model_path.exists():\n        state = torch.load(str(model_path))\n        epoch = state['epoch']\n        step = state['step']\n        model.load_state_dict(state['model'])\n        print('Restored model, epoch {}, step {:,}'.format(epoch, step))\n        isrestore = True\n        startepoch = epoch\n        \n        valid_metrics = validation(model, criterion, valid_loader)\n        valid_loss = valid_metrics['valid_loss']\n        ious_loss = valid_metrics['ious_loss']\n        print(ious_loss)\n        best_accuracy=-ious_loss\n    else:\n        epoch = 1\n        step = 0\n\n    save = lambda ep: torch.save({\n        'model': model.state_dict(),\n        'epoch': ep,\n        'step': step,\n    }, str(model_path))\n    \n\n    report_each = 50\n    log = open('train_{fold}.log'.format(fold=fold),'at', encoding='utf8')\n    valid_losses = []\n    for epoch in range(epoch, n_epochs + 1):\n        model.train()\n        random.seed()\n        tq = tqdm(total=len(train_loader) *  BATCH_SIZE)\n        tq.set_description('Epoch {}, lr {}'.format(epoch, lr))\n        losses = []\n        tl = train_loader\n        try:\n            mean_loss = 0\n            for i, (inputs, targets) in enumerate(tl):\n                inputs, targets = variable(inputs), variable(targets)\n                outputs,o2,o3,o4,o5 = model(inputs)\n                loss = criterion(outputs, o2, o3, o4, o5, targets)\n                optimizer.zero_grad()\n                batch_size = inputs.size(0)\n                loss.backward()\n                optimizer.step()\n                step += 1\n                tq.update(batch_size)\n                losses.append(loss.data[0])\n                mean_loss = np.mean(losses[-report_each:])\n                tq.set_postfix(loss='{:.5f}'.format(mean_loss))\n                if i and i % report_each == 0:\n                    write_event(log, lr, step, loss=mean_loss)\n            write_event(log, lr, step, loss=mean_loss)########################lr\n            tq.close()\n            #save(epoch + 1)\n            valid_metrics = validation(model, criterion, valid_loader)\n            write_event(log, lr, step, **valid_metrics)#############################\n            valid_loss = valid_metrics['valid_loss']\n\n            acc= -valid_metrics['ious_loss']\n            #scheduler.step(acc)###########################\n            #lr=get_learning_rate(optimizer)###############################\n            \n            if epoch==1:\n                best_accuracy=-valid_metrics['ious_loss']#valid_loss#(valid_loss.from_numpy())\n                is_best=True\n                if is_best:\n                    print (\"=> Saving a new best\")\n                    save(epoch) # save checkpoint\n                else:\n                    print (\"=> Validation Accuracy did not improve\")\n            else:\n                #if isrestore == False:\n                is_best = bool(acc < best_accuracy)#(acc.numpy() > best_accuracy.numpy())\n                best_accuracy = min(acc, best_accuracy)#torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n                if is_best:\n                    print (\"=> Saving a new best\")\n                    save(epoch) # save checkpoint\n                else:\n                    print (\"=> Validation Accuracy did not improve\")\n                #else:\n                      \n                    \"\"\"if epoch == startepoch:\n                        best_accuracy=valid_loss\n                    else:\n                        is_best = bool(acc < best_accuracy)#(acc.numpy() > best_accuracy.numpy())\n                        best_accuracy = min(acc, best_accuracy)#torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n                        if is_best:\n                            print (\"=> Saving a new best\")\n                            save(epoch) # save checkpoint\n                        else:\n                            print (\"=> Validation Accuracy did not improve\")\"\"\"\n            valid_losses.append(valid_loss)\n        except KeyboardInterrupt:\n            tq.close()\n            print('Ctrl+C, saving snapshot')\n            save(epoch)\n            print('done.')\n            return","execution_count":null,"outputs":[]},{"metadata":{"id":"T0WH67lEsezP","colab_type":"code","colab":{},"trusted":true,"_uuid":"699219d2afc0d6b5321e41f475da10bf5fbdfcf1"},"cell_type":"code","source":"def retrain(lr, model, criterion, train_loader, valid_loader, validation, init_optimizer, n_epochs=1, fold=1):\n    optimizer = init_optimizer(lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.5,patience=20)################\n    #model = nn.DataParallel(model, device_ids=None)\n    if torch.cuda.is_available():\n        model.cuda()\n    isrestore = False   \n    startepoch = 0\n    model_path = Path('model_{fold}.pt'.format(fold=fold))\n    if model_path.exists():\n        state = torch.load(str(model_path))\n        epoch = state['epoch']\n        step = state['step']\n        model.load_state_dict(state['model'])\n        print('Restored model, epoch {}, step {:,}'.format(epoch, step))\n        isrestore = True\n        startepoch = epoch\n        \n        valid_metrics = validation(model, criterion, valid_loader)\n        valid_loss = valid_metrics['valid_loss']\n        ious_loss = valid_metrics['ious_loss']\n        print(ious_loss)\n        best_accuracy=-ious_loss\n    else:\n        epoch = 2\n        step = 0\n        valid_metrics = validation(model, criterion, valid_loader)\n        valid_loss = valid_metrics['valid_loss']\n        #print(valid_loss)\n        #best_accuracy=valid_loss\n        ious_loss = valid_metrics['ious_loss']\n        print(ious_loss)\n        best_accuracy=-ious_loss        \n\n    save = lambda ep: torch.save({\n        'model': model.state_dict(),\n        'epoch': ep,\n        'step': step,\n    }, str(model_path))\n\n    print (\"=> Saving a new best\")###############################\n    save(epoch) # save checkpoint########################\n\n    report_each = 50\n    log = open('train_{fold}.log'.format(fold=fold),'at', encoding='utf8')\n    valid_losses = []\n    for epoch in range(epoch, n_epochs + 1):\n        model.train()\n        random.seed()\n        tq = tqdm(total=len(train_loader) *  BATCH_SIZE)\n        tq.set_description('Epoch {}, lr {}'.format(epoch, lr))\n        losses = []\n        tl = train_loader\n        try:\n            mean_loss = 0\n            for i, (inputs, targets) in enumerate(tl):\n                inputs, targets = variable(inputs), variable(targets)\n                outputs = model(inputs)\n                #outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                optimizer.zero_grad()\n                batch_size = inputs.size(0)\n                loss.backward()\n                optimizer.step()\n                step += 1\n                tq.update(batch_size)\n                losses.append(loss.data[0])\n                mean_loss = np.mean(losses[-report_each:])\n                tq.set_postfix(loss='{:.5f}'.format(mean_loss))\n                if i and i % report_each == 0:\n                    write_event(log, lr, step, loss=mean_loss)######################\n            write_event(log, lr, step, loss=mean_loss)########################\n            tq.close()\n            #save(epoch + 1)\n            valid_metrics = validation(model, criterion, valid_loader)\n            write_event(log, lr, step, **valid_metrics)##############################\n            valid_loss = valid_metrics['valid_loss']\n\n            acc= -valid_metrics['ious_loss']\n            scheduler.step(acc)###########################\n            lr=get_learning_rate(optimizer)###############################\n            \n            if epoch==1:\n                best_accuracy=-valid_metrics['ious_loss']#valid_loss#(valid_loss.from_numpy())\n                is_best=True\n                if is_best:\n                    print (\"=> Saving a new best\")\n                    save(epoch) # save checkpoint\n                else:\n                    print (\"=> Validation Accuracy did not improve\")\n            else:\n                #if isrestore == False:\n                is_best = bool(acc < best_accuracy)#(acc.numpy() > best_accuracy.numpy())\n                best_accuracy = min(acc, best_accuracy)#torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n                if is_best:\n                    print (\"=> Saving a new best\")\n                    save(epoch) # save checkpoint\n                else:\n                    print (\"=> Validation Accuracy did not improve\")\n                #else:\n                      \n                    \"\"\"if epoch == startepoch:\n                        best_accuracy=valid_loss\n                    else:\n                        is_best = bool(acc < best_accuracy)#(acc.numpy() > best_accuracy.numpy())\n                        best_accuracy = min(acc, best_accuracy)#torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n                        if is_best:\n                            print (\"=> Saving a new best\")\n                            save(epoch) # save checkpoint\n                        else:\n                            print (\"=> Validation Accuracy did not improve\")\"\"\"\n            valid_losses.append(valid_loss)\n        except KeyboardInterrupt:\n            tq.close()\n            print('Ctrl+C, saving snapshot')\n            save(epoch)\n            print('done.')\n            return\n\ndef retrain4(lr, model, train_loader, valid_loader,criterion, validation, init_optimizer, n_epochs=1, fold=1):\n    optimizer = init_optimizer(lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.5,patience=20)################\n    #model = nn.DataParallel(model, device_ids=None)\n    if torch.cuda.is_available():\n        model.cuda()\n    isrestore = False   \n    startepoch = 0\n    model_path = Path('model_{fold}.pt'.format(fold=fold))\n    if model_path.exists():\n        state = torch.load(str(model_path))\n        epoch = state['epoch']\n        step = state['step']\n        model.load_state_dict(state['model'])\n        print('Restored model, epoch {}, step {:,}'.format(epoch, step))\n        isrestore = True\n        startepoch = epoch\n        \n        valid_metrics = validation(model, criterion, valid_loader)\n        valid_loss = valid_metrics['valid_loss']\n        ious_loss = valid_metrics['ious_loss']\n        print(ious_loss)\n        best_accuracy=-ious_loss\n    else:\n        epoch = 2\n        step = 0\n        valid_metrics = validation(model, criterion, valid_loader)\n        valid_loss = valid_metrics['valid_loss']\n        #print(valid_loss)\n        #best_accuracy=valid_loss\n        ious_loss = valid_metrics['ious_loss']\n        print(ious_loss)\n        best_accuracy=-ious_loss        \n\n    save = lambda ep: torch.save({\n        'model': model.state_dict(),\n        'epoch': ep,\n        'step': step,\n    }, str(model_path))\n\n    print (\"=> Saving a new best\")###############################\n    save(epoch) # save checkpoint########################\n\n    report_each = 50\n    log = open('train_{fold}.log'.format(fold=fold),'at', encoding='utf8')\n    valid_losses = []\n    for epoch in range(epoch, n_epochs + 1):\n        model.train()\n        random.seed()\n        tq = tqdm(total=len(train_loader) *  BATCH_SIZE)\n        tq.set_description('Epoch {}, lr {}'.format(epoch, lr))\n        losses = []\n        tl = train_loader\n        try:\n            mean_loss = 0\n            for i, (inputs, targets) in enumerate(tl):\n                inputs, targets = variable(inputs), variable(targets)\n                outputs,o2,o3 ,o4= model(inputs)\n                #outputs = model(inputs)\n                loss = criterion(outputs, o2,o3,o4, targets)\n                optimizer.zero_grad()\n                batch_size = inputs.size(0)\n                loss.backward()\n                optimizer.step()\n                step += 1\n                tq.update(batch_size)\n                losses.append(loss.data[0])\n                mean_loss = np.mean(losses[-report_each:])\n                tq.set_postfix(loss='{:.5f}'.format(mean_loss))\n                if i and i % report_each == 0:\n                    write_event(log, lr, step, loss=mean_loss)######################\n            write_event(log, lr, step, loss=mean_loss)########################\n            tq.close()\n            #save(epoch + 1)\n            valid_metrics = validation(model, criterion, valid_loader)\n            write_event(log, lr, step, **valid_metrics)##############################\n            valid_loss = valid_metrics['valid_loss']\n\n            acc= -valid_metrics['ious_loss']\n            #scheduler.step(acc)###########################\n            #lr=get_learning_rate(optimizer)###############################\n            \n            if epoch==1:\n                best_accuracy=-valid_metrics['ious_loss']#valid_loss#(valid_loss.from_numpy())\n                is_best=True\n                if is_best:\n                    print (\"=> Saving a new best\")\n                    save(epoch) # save checkpoint\n                else:\n                    print (\"=> Validation Accuracy did not improve\")\n            else:\n                #if isrestore == False:\n                is_best = bool(acc < best_accuracy)#(acc.numpy() > best_accuracy.numpy())\n                best_accuracy = min(acc, best_accuracy)#torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n                if is_best:\n                    print (\"=> Saving a new best\")\n                    save(epoch) # save checkpoint\n                else:\n                    print (\"=> Validation Accuracy did not improve\")\n                #else:\n                      \n                    \"\"\"if epoch == startepoch:\n                        best_accuracy=valid_loss\n                    else:\n                        is_best = bool(acc < best_accuracy)#(acc.numpy() > best_accuracy.numpy())\n                        best_accuracy = min(acc, best_accuracy)#torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n                        if is_best:\n                            print (\"=> Saving a new best\")\n                            save(epoch) # save checkpoint\n                        else:\n                            print (\"=> Validation Accuracy did not improve\")\"\"\"\n            valid_losses.append(valid_loss)\n        except KeyboardInterrupt:\n            tq.close()\n            print('Ctrl+C, saving snapshot')\n            save(epoch)\n            print('done.')\n            return\n\ndef retrain5(lr, model, train_loader, valid_loader,criterion, validation, init_optimizer, n_epochs=1, fold=1):\n    optimizer = init_optimizer(lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.5,patience=20)################\n    #model = nn.DataParallel(model, device_ids=None)\n    if torch.cuda.is_available():\n        model.cuda()\n    isrestore = False   \n    startepoch = 0\n    model_path = Path('model_{fold}.pt'.format(fold=fold))\n    if model_path.exists():\n        state = torch.load(str(model_path))\n        epoch = state['epoch']\n        step = state['step']\n        model.load_state_dict(state['model'])\n        print('Restored model, epoch {}, step {:,}'.format(epoch, step))\n        isrestore = True\n        startepoch = epoch\n        \n        valid_metrics = validation(model, criterion, valid_loader)\n        valid_loss = valid_metrics['valid_loss']\n        ious_loss = valid_metrics['ious_loss']\n        print(ious_loss)\n        best_accuracy=-ious_loss\n    else:\n        epoch = 2\n        step = 0\n        valid_metrics = validation(model, criterion, valid_loader)\n        valid_loss = valid_metrics['valid_loss']\n        #print(valid_loss)\n        #best_accuracy=valid_loss\n        ious_loss = valid_metrics['ious_loss']\n        print(ious_loss)\n        best_accuracy=-ious_loss        \n\n    save = lambda ep: torch.save({\n        'model': model.state_dict(),\n        'epoch': ep,\n        'step': step,\n    }, str(model_path))\n\n    print (\"=> Saving a new best\")###############################\n    save(epoch) # save checkpoint########################\n\n    report_each = 50\n    log = open('train_{fold}.log'.format(fold=fold),'at', encoding='utf8')\n    valid_losses = []\n    for epoch in range(epoch, n_epochs + 1):\n        model.train()\n        random.seed()\n        tq = tqdm(total=len(train_loader) *  BATCH_SIZE)\n        tq.set_description('Epoch {}, lr {}'.format(epoch, lr))\n        losses = []\n        tl = train_loader\n        try:\n            mean_loss = 0\n            for i, (inputs, targets) in enumerate(tl):\n                inputs, targets = variable(inputs), variable(targets)\n                outputs,o2,o3 ,o4, o5= model(inputs)\n                #outputs = model(inputs)\n                loss = criterion(outputs, o2,o3,o4,o5, targets)\n                optimizer.zero_grad()\n                batch_size = inputs.size(0)\n                loss.backward()\n                optimizer.step()\n                step += 1\n                tq.update(batch_size)\n                losses.append(loss.data[0])\n                mean_loss = np.mean(losses[-report_each:])\n                tq.set_postfix(loss='{:.5f}'.format(mean_loss))\n                if i and i % report_each == 0:\n                    write_event(log, lr, step, loss=mean_loss)######################\n            write_event(log, lr, step, loss=mean_loss)########################\n            tq.close()\n            #save(epoch + 1)\n            valid_metrics = validation(model, criterion, valid_loader)\n            write_event(log, lr, step, **valid_metrics)##############################\n            valid_loss = valid_metrics['valid_loss']\n\n            acc= -valid_metrics['ious_loss']\n            #scheduler.step(acc)###########################\n            #lr=get_learning_rate(optimizer)###############################\n            \n            if epoch==1:\n                best_accuracy=-valid_metrics['ious_loss']#valid_loss#(valid_loss.from_numpy())\n                is_best=True\n                if is_best:\n                    print (\"=> Saving a new best\")\n                    save(epoch) # save checkpoint\n                else:\n                    print (\"=> Validation Accuracy did not improve\")\n            else:\n                #if isrestore == False:\n                is_best = bool(acc < best_accuracy)#(acc.numpy() > best_accuracy.numpy())\n                best_accuracy = min(acc, best_accuracy)#torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n                if is_best:\n                    print (\"=> Saving a new best\")\n                    save(epoch) # save checkpoint\n                else:\n                    print (\"=> Validation Accuracy did not improve\")\n                #else:\n                      \n                    \"\"\"if epoch == startepoch:\n                        best_accuracy=valid_loss\n                    else:\n                        is_best = bool(acc < best_accuracy)#(acc.numpy() > best_accuracy.numpy())\n                        best_accuracy = min(acc, best_accuracy)#torch.FloatTensor(min(acc.numpy(), best_accuracy.numpy()))\n                        if is_best:\n                            print (\"=> Saving a new best\")\n                            save(epoch) # save checkpoint\n                        else:\n                            print (\"=> Validation Accuracy did not improve\")\"\"\"\n            valid_losses.append(valid_loss)\n        except KeyboardInterrupt:\n            tq.close()\n            print('Ctrl+C, saving snapshot')\n            save(epoch)\n            print('done.')\n            return","execution_count":null,"outputs":[]},{"metadata":{"id":"9Yfer8vmAs3W","colab_type":"text","_uuid":"a5df1eb6560b29eef9115def1140cb7fafbb9918"},"cell_type":"markdown","source":"# Training......"},{"metadata":{"id":"l-f3j9RCsezR","colab_type":"code","colab":{},"trusted":true,"_uuid":"086b7a53a4abb579ef2a8571057ca199e8982feb"},"cell_type":"code","source":"def make_loader(in_df, batch_size, shuffle=False, transform=None):\n        return DataLoader(\n            dataset=SaltDataset(in_df, transform=transform),\n            shuffle=shuffle,\n            num_workers = 0,\n            batch_size = batch_size,\n            pin_memory=torch.cuda.is_available()\n        )\n\ntrain_loader = make_loader(train_df, batch_size =  BATCH_SIZE, shuffle=True, transform=train_transform)\nvalid_loader = make_loader(valid_df, batch_size = BATCH_SIZE // 2, transform=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f33872799ea200a1576cc389594eb04b4e523c39"},"cell_type":"code","source":"model00 = LinkNet34deeps(1)#\nmodel01 = LinkNet34deeps(1)#\nmodel02 = LinkNet34deeps(1)#\n\nmodel10 = LinkNet34deeps(1)#\nmodel11 = LinkNet34deeps(1)#\nmodel12 = LinkNet34deeps(1)#\n\nmodel20 = LinkNet34deeps(1)#\nmodel21 = LinkNet34deeps(1)#\nmodel22 = LinkNet34deeps(1)#\n\nmodel30 = LinkNet34deeps(1)#\nmodel31 = LinkNet34deeps(1)#\nmodel32 = LinkNet34deeps(1)#\n\nmodel40 = LinkNet34deeps(1)#\nmodel41 = LinkNet34deeps(1)#\nmodel42 = LinkNet34deeps(1)#\n#model5 = DenseNet34(1)#\n\nmodel_path00 ='../input/linknet34deepsv3fulldacv0re6scalere2/model_1.pt'\nmodel_path01 ='../input/linknet34deepsv3cv0scaleclr/model_1.pt'\nmodel_path02 ='../input/linknet34deepsv3cv0scaleclrre/model_1.pt'\n\nmodel_path10 ='../input/linknet34deepsv3fulldacv1scalere2001/model_1.pt'\nmodel_path11 ='../input/linknet34deepsv3cv1scalereclrre2/model_1.pt'\nmodel_path12 ='../input/linknet34deepsv3cv1scalereclrre3/model_1.pt'\n\nmodel_path20 ='../input/linknet34deepsv3fulldacv2scalere2001/model_1.pt'\nmodel_path21 ='../input/linknet34deepsv3cv2scaleclrre/model_1.pt'\nmodel_path22 ='../input/linknet34deepsv3cv2scaleclrre3/model_1.pt'\n\nmodel_path30 ='../input/linknet34deepsv3cv3scaleclrre/model_1.pt'\nmodel_path31 ='../input/linknet34deepsv3cv3scaleclrre2/model_1.pt'\nmodel_path32 ='../input/linknet34deepsv3cv3scaleclrre3/model_1.pt'\n\nmodel_path40 ='../input/linknet34deepsv3cv4scaleclr/model_1.pt'\nmodel_path41 ='../input/linknet34deepsv3cv4scaleclrre/model_1.pt'\nmodel_path42 ='../input/linknet34deepsv3cv4scaleclrre3/model_1.pt'\n\n#model_path5 ='../input/hyper0924cv5re/model_1.pt'\n\nstate00 = torch.load(str(model_path00))\nstate00 = {key.replace('module.', ''): value for key, value in state00['model'].items()}\nmodel00.load_state_dict(state00)\nif torch.cuda.is_available():\n    model00.cuda()\nmodel00.eval()\n\nstate01 = torch.load(str(model_path01))\nstate01 = {key.replace('module.', ''): value for key, value in state01['model'].items()}\nmodel01.load_state_dict(state01)\nif torch.cuda.is_available():\n    model01.cuda()\nmodel01.eval()\n\nstate02 = torch.load(str(model_path02))\nstate02 = {key.replace('module.', ''): value for key, value in state02['model'].items()}\nmodel02.load_state_dict(state02)\nif torch.cuda.is_available():\n    model02.cuda()\nmodel02.eval()\n\nstate10 = torch.load(str(model_path10))\nstate10 = {key.replace('module.', ''): value for key, value in state10['model'].items()}\nmodel10.load_state_dict(state10)\nif torch.cuda.is_available():\n    model10.cuda()\nmodel10.eval()\n\nstate11 = torch.load(str(model_path11))\nstate11 = {key.replace('module.', ''): value for key, value in state11['model'].items()}\nmodel11.load_state_dict(state11)\nif torch.cuda.is_available():\n    model11.cuda()\nmodel11.eval()\n\nstate12 = torch.load(str(model_path12))\nstate12 = {key.replace('module.', ''): value for key, value in state12['model'].items()}\nmodel12.load_state_dict(state12)\nif torch.cuda.is_available():\n    model12.cuda()\nmodel12.eval()\n\n\nstate20 = torch.load(str(model_path20))\nstate20 = {key.replace('module.', ''): value for key, value in state20['model'].items()}\nmodel20.load_state_dict(state20)\nif torch.cuda.is_available():\n    model20.cuda()\nmodel20.eval()\n\nstate21 = torch.load(str(model_path21))\nstate21 = {key.replace('module.', ''): value for key, value in state21['model'].items()}\nmodel21.load_state_dict(state21)\nif torch.cuda.is_available():\n    model21.cuda()\nmodel21.eval()\n\nstate22 = torch.load(str(model_path22))\nstate22 = {key.replace('module.', ''): value for key, value in state22['model'].items()}\nmodel22.load_state_dict(state22)\nif torch.cuda.is_available():\n    model22.cuda()\nmodel22.eval()\n\nstate30 = torch.load(str(model_path30))\nstate30 = {key.replace('module.', ''): value for key, value in state30['model'].items()}\nmodel30.load_state_dict(state30)\nif torch.cuda.is_available():\n    model30.cuda()\nmodel30.eval()\n\nstate31 = torch.load(str(model_path31))\nstate31 = {key.replace('module.', ''): value for key, value in state31['model'].items()}\nmodel31.load_state_dict(state31)\nif torch.cuda.is_available():\n    model31.cuda()\nmodel31.eval()\n\nstate32 = torch.load(str(model_path32))\nstate32 = {key.replace('module.', ''): value for key, value in state32['model'].items()}\nmodel32.load_state_dict(state32)\nif torch.cuda.is_available():\n    model32.cuda()\nmodel32.eval()\n\nstate40 = torch.load(str(model_path40))\nstate40 = {key.replace('module.', ''): value for key, value in state40['model'].items()}\nmodel40.load_state_dict(state40)\nif torch.cuda.is_available():\n    model40.cuda()\nmodel40.eval()\n\nstate41 = torch.load(str(model_path41))\nstate41 = {key.replace('module.', ''): value for key, value in state41['model'].items()}\nmodel41.load_state_dict(state41)\nif torch.cuda.is_available():\n    model41.cuda()\nmodel41.eval()\n\nstate42 = torch.load(str(model_path42))\nstate42 = {key.replace('module.', ''): value for key, value in state42['model'].items()}\nmodel42.load_state_dict(state42)\nif torch.cuda.is_available():\n    model42.cuda()\nmodel42.eval()","execution_count":null,"outputs":[]},{"metadata":{"id":"c5PFFIFWsezU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":5057},"outputId":"b378914d-9f02-4fbc-c00b-96ed92a2db67","executionInfo":{"status":"ok","timestamp":1537790050915,"user_tz":-480,"elapsed":2361752,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"18b1b40b443f136428488782d5e851e4af5f7a22"},"cell_type":"markdown","source":"#train3(init_optimizer=lambda lr: torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters())),#(model.parameters(), lr=lr),\n#train4(init_optimizer=lambda lr: torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-2, momentum=0.9, weight_decay=0.0001),#(model.parameters(), lr=lr),\ntrain4(init_optimizer=lambda lr: torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters())),#(model.parameters(), lr=lr),\n        lr = 5e-4,#1e-4\n        n_epochs = 30,#100,#35,#40,\n        model=model,\n        criterion=criterion,#LossBinaryElu(jaccard_weight=0),\n        train_loader=train_loader,\n        valid_loader=valid_loader,\n        validation=validation4,\n     )"},{"metadata":{"trusted":true,"_uuid":"c2f096d010d17076761ce2b3f1f61426d5f95f2b"},"cell_type":"code","source":"def criterionL(output_4, output_3, output_2, output_1, truth_pixel, is_average=True):\n    #print(torch.sum(torch.sum(truth_pixel)))\n    bat,dim,m,n=truth_pixel.shape\n\n\n    loss_logit1 = lovasz_loss(output_1, truth_pixel)#1-1-1\n    loss_logit2 = lovasz_loss(output_2, truth_pixel)#1-1-1\n    loss_logit3 = lovasz_loss(output_3, truth_pixel)#1-1-1\n    loss_logit4 = lovasz_loss(output_4, truth_pixel)#1-1-1\n\n    #weight_image, weight_pixel = 0.1, 10  #focal\n    weight_1, weight_2, weight_3, weight_4 = 0.2, 0.2, 0.2, 1  #lovasz?\n    #weight_image, weight_pixel = 0.1, 2 #bce\n\n\n\n    return weight_1*loss_logit1+ weight_2*loss_logit2+weight_3*loss_logit3+weight_4*loss_logit4\n\ndef criterionL5(output_4, output_3, output_2, output_1, logit_image, truth_pixel, is_average=True):\n    #print(torch.sum(torch.sum(truth_pixel)))\n    bat,dim,m,n=truth_pixel.shape\n\n    truth_image = torch.Tensor(bat)\n    i=0\n    for x in truth_pixel:\n        truth_image[i]=(x.squeeze().sum()>0).cuda().float()\n        i=i+1\n\n    truth_image=truth_image.type(torch.cuda.FloatTensor)\n\n    loss_image = F.binary_cross_entropy_with_logits(logit_image, truth_image, reduce=is_average)#1-1-1\n    \n    loss_logit1 = lovasz_loss(output_1, truth_pixel)#1-1-1\n    loss_logit2 = lovasz_loss(output_2, truth_pixel)#1-1-1\n    loss_logit3 = lovasz_loss(output_3, truth_pixel)#1-1-1\n    loss_logit4 = lovasz_loss(output_4, truth_pixel)#1-1-1\n\n    #weight_image, weight_pixel = 0.1, 10  #focal\n    weight_1, weight_2, weight_3, weight_4, weight_image = 0.2, 0.2, 0.2, 1, 0.05  #0.1, 0.1, 0.1, 1, 0.05  #0.2, 0.2, 0.2, 1, 0.05  #lovasz?\n    #weight_image, weight_pixel = 0.1, 2 #bce\n\n\n\n    return weight_1*loss_logit1+ weight_2*loss_logit2+weight_3*loss_logit3+weight_4*loss_logit4+weight_image*loss_image","execution_count":null,"outputs":[]},{"metadata":{"id":"OQdhAHQ5sezu","colab_type":"text","_uuid":"9c09e7f937590e7a56fca89bfdca89b95cf02383"},"cell_type":"markdown","source":"# predict and show prediction \nimg,_ = valid_ds[0]\ninput_img = torch.unsqueeze(variable(img, volatile=True), dim=0)\nmask = (F.sigmoid(model(input_img)))\nout_mask = torch.squeeze(mask.data.cpu(), dim = 0)\nimshow(img,out_mask)"},{"metadata":{"id":"wNorMuKodn6m","colab_type":"code","colab":{},"trusted":true,"_uuid":"90c70726ad2c62c6afb6e6e4ac2c83f19b6c131f"},"cell_type":"code","source":"def dummy_prediction(X_test,model,mirror=True):\n    #X_test0 = variable(X_test, volatile=True)\n    # ...\n    y_preds = model(X_test)\n    y_preds = F.sigmoid(y_preds).data.cpu().numpy()\n    if mirror==True:\n        #print(X_test.shape)#32-3-128-128\n        m_preds = model(variable(torch.FloatTensor((X_test.cpu().data.numpy()[:,:,:,::-1]).copy()), volatile=True))\n        ##m_preds = model(Variable(torch.from_numpy(X_test.cpu().data.numpy()[:,:,::-1,:])))\n        m_preds = F.sigmoid(variable(torch.FloatTensor((m_preds.cpu().data.numpy()[:,:,:,::-1]).copy()), volatile=True)).data.cpu().numpy()\n        y_preds = 0.5 * (y_preds + m_preds)\n        #y_preds = 0.5 * (y_preds + variable(torch.FloatTensor((m_preds.cpu().data.numpy()[:,:,::-1,:]).copy()), volatile=True))\n    return y_preds\ndef dummy_prediction4(X_test,model,mirror=True):\n    #X_test0 = variable(X_test, volatile=True)\n    # ...\n    y_preds,o2,o3,o4 = model(X_test)\n    y_preds = F.sigmoid(y_preds).data.cpu().numpy()\n    if mirror==True:\n        #print(X_test.shape)#32-3-128-128\n        m_preds,o2,o3,o4 = model(variable(torch.FloatTensor((X_test.cpu().data.numpy()[:,:,:,::-1]).copy()), volatile=True))\n        ##m_preds = model(Variable(torch.from_numpy(X_test.cpu().data.numpy()[:,:,::-1,:])))\n        m_preds = F.sigmoid(variable(torch.FloatTensor((m_preds.cpu().data.numpy()[:,:,:,::-1]).copy()), volatile=True)).data.cpu().numpy()\n        y_preds = 0.5 * (y_preds + m_preds)\n        #y_preds = 0.5 * (y_preds + variable(torch.FloatTensor((m_preds.cpu().data.numpy()[:,:,::-1,:]).copy()), volatile=True))\n    return y_preds\ndef dummy_prediction2(X_test,model,mirror=True):\n    #X_test0 = variable(X_test, volatile=True)\n    # ...\n    y_preds,o2,o3,o4,o5 = model(X_test)\n    y_preds = F.sigmoid(y_preds).data.cpu().numpy()\n    if mirror==True:\n        #print(X_test.shape)#32-3-128-128\n        m_preds,o2,o3,o4,o5 = model(variable(torch.FloatTensor((X_test.cpu().data.numpy()[:,:,:,::-1]).copy()), volatile=True))\n        ##m_preds = model(Variable(torch.from_numpy(X_test.cpu().data.numpy()[:,:,::-1,:])))\n        m_preds = F.sigmoid(variable(torch.FloatTensor((m_preds.cpu().data.numpy()[:,:,:,::-1]).copy()), volatile=True)).data.cpu().numpy()\n        y_preds = 0.5 * (y_preds + m_preds)\n        #y_preds = 0.5 * (y_preds + variable(torch.FloatTensor((m_preds.cpu().data.numpy()[:,:,::-1,:]).copy()), volatile=True))\n    return y_preds","execution_count":null,"outputs":[]},{"metadata":{"id":"tLBmpQzGsezv","colab_type":"text","_uuid":"eb64fe1ee23c96af5ef65ec4fc33b489772a5947"},"cell_type":"markdown","source":"## Prediction for Test images\n\nFor test purposes predict only for 5000 images. Prediction of the full set of images (88500 images) takes more than 6hr !!!"},{"metadata":{"id":"porXHuq4sezx","colab_type":"text","_uuid":"e9c719c2c93babc830fbfe08c1d0554bb8a1e75c"},"cell_type":"markdown","source":"test_paths = os.listdir(test_image_dir+'/images/')\nprint(len(test_paths), 'test images found')"},{"metadata":{"id":"GQHp3ZKLim8F","colab_type":"code","colab":{},"trusted":true,"_uuid":"fe54d65d64b69fe5ababf3d007301d4c13101b27"},"cell_type":"markdown","source":"train_df1=train_df1.set_index('id')"},{"metadata":{"id":"i5DEQlOFsezy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":271},"outputId":"4dee93ca-45da-4045-f470-bbd5f5264be0","executionInfo":{"status":"ok","timestamp":1537791947369,"user_tz":-480,"elapsed":12620,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"48de175c6324dad51f8e16eed2b9b67555c7cd34"},"cell_type":"code","source":"#train_df1=train_df1.set_index('id')\n\n\nfrom skimage.morphology import binary_opening, disk\n\nloader = DataLoader(\n        dataset=SaltDataset(valid_df, transform=None, mode='valid'),###########################################################\n        shuffle=False,\n        batch_size=BATCH_SIZE,\n        num_workers=0,\n        pin_memory=torch.cuda.is_available()\n    ) \n\ny_valid_ori = []\n#preds_valid = [] #\nout_pred_rows = []#pd.DataFrame(columns=('id', 'mask'))#[]\nfor batch_num, (inputs, paths) in enumerate(tqdm(loader, desc='valid')):\n    inputs = variable(inputs, volatile=True)\n    #outputs = dummy_prediction2(inputs,model)\n    \n    outputs0 = dummy_prediction2(inputs,model00)\n    outputs0 = outputs0+dummy_prediction2(inputs,model01)\n    outputs0 = outputs0+dummy_prediction2(inputs,model02)\n    \n    outputs1 = dummy_prediction2(inputs,model10)\n    outputs1 = outputs1+dummy_prediction2(inputs,model11)\n    outputs1 = outputs1+dummy_prediction2(inputs,model12)\n    \n    outputs2 = dummy_prediction2(inputs,model20)\n    outputs2 = outputs2+dummy_prediction2(inputs,model21)\n    outputs2 = outputs2+dummy_prediction2(inputs,model22)\n    \n    outputs3 = dummy_prediction2(inputs,model30)\n    outputs3 = outputs3+dummy_prediction2(inputs,model31)\n    outputs3 = outputs3+dummy_prediction2(inputs,model32)\n    \n    outputs4 = dummy_prediction2(inputs,model40)\n    outputs4 = outputs4+dummy_prediction2(inputs,model41)\n    outputs4 = outputs4+dummy_prediction2(inputs,model42)\n    #outputs = model(inputs)\n    #y_valid_ori=[downsample(crop(F.sigmoid(outputs[i,0]).data.cpu().numpy())) for for i, image_name in enumerate(paths)]\n    for i, image_name in enumerate(paths):\n        #mask = outputs[i,0]#F.sigmoid(outputs[i,0]).data.cpu().numpy()\n        \n        mask0 = outputs0[i,0]#F.sigmoid(outputs0[i,0]).data.cpu().numpy()\n        mask1 = outputs1[i,0]#F.sigmoid(outputs1[i,0]).data.cpu().numpy()\n        mask2 = outputs2[i,0]#F.sigmoid(outputs2[i,0]).data.cpu().numpy()\n        mask3 = outputs3[i,0]#F.sigmoid(outputs3[i,0]).data.cpu().numpy()\n        mask4 = outputs4[i,0]#F.sigmoid(outputs4[i,0]).data.cpu().numpy()\n        #mask5 = outputs5[i,0]#F.sigmoid(outputs5[i,0]).data.cpu().numpy()\n        mask = (mask0+mask1+mask2+mask3+mask4)/15\n        \n        out_pred_rows.extend([restore(mask).tolist()])# += [{'id': image_name, 'mask': mask}]\n        y_valid_ori.extend([train_df1.loc[image_name].masks])\n        #mask = F.sigmoid(torch.FloatTensor(restore(outputs[i,0].data.cpu().numpy())))\n        #out_pred_rows.extend([mask.tolist()])# += [{'id': image_name, 'mask': mask}]\n        #y_valid_ori.extend([train_df1.loc[image_name].masks])\n\n\"\"\"  if i==1:\n            plt.imshow(mask)\n            print(np.max(np.max(outputs[i,0].data.cpu().numpy())))\n            print(mask.shape)\n        cur_seg = binary_opening(mask>0.5, disk(2))\n        cur_rles = multi_rle_encode(cur_seg)\n        if len(cur_rles)>0:\n            for c_rle in cur_rles:\n                #out_pred_rows += [{'ImageId': image_name, 'EncodedPixels': c_rle}]\n                out_pred_rows += [{'id': image_name, 'rle_mask': c_rle}]\n        else:\n            #out_pred_rows += [{'ImageId': image_name, 'EncodedPixels': None}]\n            out_pred_rows += [{'id': image_name, 'rle_mask': None}]\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"id":"3uwZtOc9ikxL","colab_type":"text","_uuid":"7997ae94b794ce1517063491a7278992677e8c09"},"cell_type":"markdown","source":""},{"metadata":{"id":"KgACIf_Msez2","colab_type":"code","colab":{},"trusted":true,"_uuid":"1f00d57422a55ab95ffa6a641894b7972947836b"},"cell_type":"code","source":"y_valid_ori = np.array(y_valid_ori)\npreds_valid = np.array(out_pred_rows)","execution_count":null,"outputs":[]},{"metadata":{"id":"aLy_n6d9sez-","colab_type":"text","_uuid":"926a0de5a9d7d2f20a5702d756c46316006f90ec"},"cell_type":"markdown","source":"#out_pred_rows=pd.DataFrame(out_pred_rows)[['id', 'mask']]#=pd.DataFrame(out_pred_rows)\nout_pred_rows=out_pred_rows.set_index('id')\npreds_valid=np.array([out_pred_rows.loc[idx].mask for idx in ids_valid.id])\n#train_df0=train_df0.set_index('id')\n#y_valid_ori = np.array([train_df0.loc[idx].masks for idx in ids_valid.id])"},{"metadata":{"id":"QERYA74gsez_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b8f57969-1379-4441-a1ce-457ea7f99755","executionInfo":{"status":"ok","timestamp":1537791952060,"user_tz":-480,"elapsed":661,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"156f200cdd63b49061a8ecab9def7cd50e8a99d4"},"cell_type":"code","source":"preds_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"FNgQ9CIYse0C","colab_type":"code","colab":{},"trusted":true,"_uuid":"893605d81c8cb9d0aafe9be0c6fac58ed4ddfe0b"},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"id":"JCPot1LKse0G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":953},"outputId":"3c5be0d0-eec6-4374-cc19-43865a520c1e","executionInfo":{"status":"ok","timestamp":1537791988326,"user_tz":-480,"elapsed":31793,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"45af46637b0ee3f18852436cbb60f9c200980625"},"cell_type":"code","source":"#thresholds = np.linspace(0, 1, 50)\n#ious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm(thresholds)])\nthresholds = np.linspace(0, 1, 50)\nious = np.array([get_iou_vector(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"id":"WMUro3mSse0K","colab_type":"code","colab":{},"trusted":true,"_uuid":"cc39bbc282630ad258f18d93e3f73d647d970d6c"},"cell_type":"code","source":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","execution_count":null,"outputs":[]},{"metadata":{"id":"_cvgEOP9se0M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":395},"outputId":"509dea51-405c-418f-9931-17cbe4134f97","executionInfo":{"status":"ok","timestamp":1537791990675,"user_tz":-480,"elapsed":1060,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"5ec5f2322b1d0be80d35e0b268dbe790d4c57579"},"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"id":"1aFDEtBase0P","colab_type":"code","colab":{},"trusted":true,"_uuid":"2055a0b78f400b4d1b5892216c18854961ba7733"},"cell_type":"code","source":"# Source https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","execution_count":null,"outputs":[]},{"metadata":{"id":"iRFsAoAhse0U","colab_type":"code","colab":{},"trusted":true,"_uuid":"093a281e003215e1f29824ea6953307bfd574952"},"cell_type":"code","source":"#x_test = np.array([imgexpand(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"SC3pRU6fse0X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"345a80a0-e803-49e6-9b66-ab458aaa8924","executionInfo":{"status":"ok","timestamp":1537791994808,"user_tz":-480,"elapsed":818,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"8992cf11e12716c6412670a457e798493b521ec4"},"cell_type":"code","source":"test_paths = os.listdir(test_image_dir+'/images/')\nprint(len(test_paths), 'test images found')","execution_count":null,"outputs":[]},{"metadata":{"id":"GU0OeZZkse0d","colab_type":"code","colab":{},"trusted":true,"_uuid":"2fc2b18fdf5c58e5f0b960fcc39fae8d828f38b0"},"cell_type":"code","source":"#test_df1 = pd.DataFrame({'id': test_paths, 'rle_mask':None})","execution_count":null,"outputs":[]},{"metadata":{"id":"27l-xmpsse0g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"16e4e544-28ef-4e0a-f4d8-a8ae503db337","executionInfo":{"status":"ok","timestamp":1537791996875,"user_tz":-480,"elapsed":827,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"e7f493e408e2bed70c44a94d016f14983da49cf7"},"cell_type":"code","source":"test_df0['rle_mask'] = None","execution_count":null,"outputs":[]},{"metadata":{"id":"IupJrzw1jxcx","colab_type":"code","colab":{},"trusted":true,"_uuid":"74a12b7cb8908e2181eef1148915461c3bb88d81"},"cell_type":"code","source":"train_df=0\ntrain_df0=0\ntrain_df1=0\ngrp=0\n","execution_count":null,"outputs":[]},{"metadata":{"id":"vgFqdm5jse0i","colab_type":"code","colab":{},"trusted":true,"_uuid":"24e962f77d7ccd5bc84fe2179eeb98de487bc511"},"cell_type":"code","source":"if not('id' in test_df0.columns.values.tolist()):\n    test_df0.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"C2MG5jKQse0k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":10225},"outputId":"51ec19b9-2dd6-4263-d6b1-00c24a75b323","executionInfo":{"status":"ok","timestamp":1537797399352,"user_tz":-480,"elapsed":5400984,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"5f7a84dab79240a342a3ec27c00d7ba42d325567"},"cell_type":"code","source":"#preds_test = model.predict(x_test)#####################################\n#test_df1 = pd.DataFrame({'id': test_paths, 'rle_mask':None})\nfrom skimage.morphology import binary_opening, disk\n#test_df=test_df[:5000]\nloader = DataLoader(\n        dataset=SaltDataset(test_df0, transform=None, mode='predict'),\n        shuffle=False,\n        batch_size=BATCH_SIZE,\n        num_workers=0,\n        pin_memory=torch.cuda.is_available()\n    ) \npreds_test = {}#[] \n#ind = []\nout_pred_rows = []\nfor batch_num, (inputs, paths) in enumerate(tqdm(loader, desc='Predict')):\n    inputs = variable(inputs, volatile=True)\n    #outputs = dummy_prediction2(inputs,model)\n    \n    outputs0 = dummy_prediction2(inputs,model00)\n    outputs0 = outputs0+dummy_prediction2(inputs,model01)\n    outputs0 = outputs0+dummy_prediction2(inputs,model02)\n    \n    outputs1 = dummy_prediction2(inputs,model10)\n    outputs1 = outputs1+dummy_prediction2(inputs,model11)\n    outputs1 = outputs1+dummy_prediction2(inputs,model12)\n    \n    outputs2 = dummy_prediction2(inputs,model20)\n    outputs2 = outputs2+dummy_prediction2(inputs,model21)\n    outputs2 = outputs2+dummy_prediction2(inputs,model22)\n    \n    outputs3 = dummy_prediction2(inputs,model30)\n    outputs3 = outputs3+dummy_prediction2(inputs,model31)\n    outputs3 = outputs3+dummy_prediction2(inputs,model32)\n    \n    outputs4 = dummy_prediction2(inputs,model40)\n    outputs4 = outputs4+dummy_prediction2(inputs,model41)\n    outputs4 = outputs4+dummy_prediction2(inputs,model42)\n    \n    #outputs = model(inputs)\n    for i, image_name in enumerate(paths):\n        #mask = F.sigmoid(torch.FloatTensor(restore(outputs[i,0].data.cpu().numpy())))\n        #preds_test[image_name[:-4]]=(mask)\n        #mask = outputs[i,0]#F.sigmoid(outputs[i,0]).data.cpu().numpy()\n        \n        mask0 = outputs0[i,0]#F.sigmoid(outputs0[i,0]).data.cpu().numpy()\n        mask1 = outputs1[i,0]#F.sigmoid(outputs1[i,0]).data.cpu().numpy()\n        mask2 = outputs2[i,0]#F.sigmoid(outputs2[i,0]).data.cpu().numpy()\n        mask3 = outputs3[i,0]#F.sigmoid(outputs3[i,0]).data.cpu().numpy()\n        mask4 = outputs4[i,0]#F.sigmoid(outputs4[i,0]).data.cpu().numpy()\n        #mask5 = outputs5[i,0]#F.sigmoid(outputs5[i,0]).data.cpu().numpy()\n        mask = (mask0+mask1+mask2+mask3+mask4)/15\n                \n        if image_name[-4:]=='.png':\n            preds_test[image_name[:-4]]=(restore(mask))\n        else:\n            preds_test[image_name]=(restore(mask))\n\"\"\" cur_seg = binary_opening(mask>threshold_best, disk(2))\n        cur_rles = multi_rle_encode(cur_seg)\n        if len(cur_rles)>0:\n            for c_rle in cur_rles:\n                #out_pred_rows += [{'ImageId': image_name, 'EncodedPixels': c_rle}]\n                out_pred_rows += [{'id': image_name, 'rle_mask': c_rle}]\n        else:\n            #out_pred_rows += [{'ImageId': image_name, 'EncodedPixels': None}]\n            out_pred_rows += [{'id': image_name, 'rle_mask': None}]\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"id":"bOWazkVese0m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"d51b0397-3e03-49bc-e44e-9481716f4ab3","executionInfo":{"status":"ok","timestamp":1537797406907,"user_tz":-480,"elapsed":6507,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"67c2b983055297f0fd0946433abd601c1ec1a8ed"},"cell_type":"code","source":"co=0\nfor i,idx in enumerate(test_df11.index.values):\n    num=np.sum(np.sum(np.round(preds_test[idx] > threshold_best)))\n    #print(num)\n    if num<20 and num>0:\n        co=co+1\n        preds_test[idx] =np.zeros((101,101))\nprint(co)","execution_count":null,"outputs":[]},{"metadata":{"id":"HZnFscBKse0p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":65033},"outputId":"7a08d068-7b5e-4355-e6bf-11c964834201","executionInfo":{"status":"ok","timestamp":1537797810667,"user_tz":-480,"elapsed":403441,"user":{"displayName":"Wenzhao Zhao","photoUrl":"","userId":"00944584830835636551"}},"trusted":true,"_uuid":"cbbf9f2275368c8b7b0c102080587208d3cfce5c"},"cell_type":"code","source":"pred_dict = {idx: RLenc(np.round(preds_test[idx] > threshold_best)) for i, idx in enumerate(tqdm(test_df11.index.values))}#.id))}","execution_count":null,"outputs":[]},{"metadata":{"id":"fTGmt39Ase0t","colab_type":"text","_uuid":"8989aa3536ba938bc9e88bd06a480a7e30a69585"},"cell_type":"markdown","source":"for idx in test_df11.index.values:\n    if np.sum(np.sum(np.array(pred_dict[idx],dtype = 'float_')))<20:\n        pred_dict[idx]=0"},{"metadata":{"id":"t1QmYtDuse0t","colab_type":"code","colab":{},"trusted":true,"_uuid":"d3a8bf54f47e4b0e086f2cedc343ac74f4129bdb"},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submissionpth.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"W2HHk9vOse0w","colab_type":"text","_uuid":"75badb849e65a5f717f8f8e9fba1f3d18fbaa51c"},"cell_type":"markdown","source":"submission_df = pd.DataFrame(out_pred_rows)[['id', 'rle_mask']]##[['ImageId', 'EncodedPixels']]#\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df#.sample(10)"}],"metadata":{"colab":{"name":"DenseNetH128-0924depth.ipynb","version":"0.3.2","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU","language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
